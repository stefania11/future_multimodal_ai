<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Keynote: The Future of Multimodal AI Applications</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-typescript.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js" defer></script> <!-- Added for JSON highlighting -->

    <style>
        :root {
            --black: #000000;
            --white: #FFFFFF;
            --soft-pink: #FFD6E0;
            --lavender: #E0D6FF;
            --mint-green: #C1F0DB;
            --dark-gray: #333333;
            --medium-gray: #666666;
            --light-gray-bg: #f7f7f7; /* For code blocks and backgrounds */
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'DM Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }

        html {
             scroll-behavior: smooth; /* Smooth scrolling between slides */
        }

        body {
            background-color: var(--white);
            color: var(--black);
            line-height: 1.6;
            height: 100vh; /* Ensure body takes full height */
            overflow: hidden; /* Prevent body scroll, container handles it */
        }

        /* Presentation container */
        #presentation-container {
            height: 100vh;
            overflow-y: scroll; /* Enable vertical scrolling */
            scroll-snap-type: y mandatory; /* Snap scrolling to slides */
            position: relative;
        }

        .slide {
            width: 100%;
            min-height: 100vh; /* Ensure slide takes full viewport height */
            padding: 4rem 5%; /* Responsive padding */
            display: flex; /* Use flex for alignment */
            flex-direction: column;
            justify-content: center;
            align-items: center; /* Center content horizontally by default */
            scroll-snap-align: start; /* Snap to the start of the slide */
            position: relative;
            overflow: hidden; /* Prevent content overflow issues */
            background-color: var(--white); /* Default background */
        }

        .slide-content {
            max-width: 1200px;
            width: 100%;
            z-index: 1; /* Content above background */
            text-align: left; /* Default text alignment */
        }
         /* Center content vertically and horizontally within slide-content if needed */
        .center-content .slide-content {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center; /* Center text for these slides */
        }


        /* Optional subtle gradient background for slides */
        .gradient-bg {
            background: linear-gradient(135deg, rgba(255, 214, 224, 0.1), rgba(224, 214, 255, 0.1), rgba(193, 240, 219, 0.1));
        }

        /* Headings */
        h1, h2, h3, h4, h5, h6 {
            margin-bottom: 1.5rem;
            font-weight: 700;
            line-height: 1.3;
        }

        h1 {
            font-size: 3.5rem !important; /* Ensure fixed size, force override */
            color: var(--dark-gray); /* Set color to dark gray */
            display: inline-block; /* Prevents taking full width */
        }

        h2 { /* Slide Titles */
            font-size: 3rem !important; /* Fixed size override */
            color: var(--black);
            padding-bottom: 0.5rem;
            position: relative;
            margin-bottom: 2rem; /* More space after title */
            display: inline-block; /* Ensure underline fits content */
        }

        /* Underline effect for h2 */
        h2::after {
             content: '';
             position: absolute;
             bottom: 0;
             left: 0;
             width: 60px;
             height: 4px;
             background: linear-gradient(90deg, var(--soft-pink), var(--lavender), var(--mint-green));
             border-radius: 2px;
         }
         /* Center underline for centered h2 */
        .center-content h2::after,
        h2.text-center::after {
            left: 50%;
            transform: translateX(-50%);
        }


        h3 { /* Subtitles / Section Heads */
            font-size: clamp(1.5rem, 4vw, 2rem);
            color: var(--dark-gray);
        }

        p, li {
            margin-bottom: 1rem;
            color: var(--dark-gray);
            font-size: clamp(1rem, 2.5vw, 1.2rem); /* Responsive font size */
        }
        ul {
            list-style-position: outside;
            padding-left: 1.5rem; /* Ensure bullets are visible */
            margin-left: 1rem; /* Indent list */
        }
        /* Make lists inside cards look better */
         .card ul {
             padding-left: 1rem;
             margin-left: 0.5rem;
         }
         .card li {
             font-size: clamp(0.9rem, 2vw, 1.1rem); /* Slightly smaller list items in cards */
             margin-bottom: 0.5rem;
         }

        /* Grid layout */
        .grid-layout { /* Renamed from 'grid' to avoid conflicts */
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); /* Responsive columns */
            gap: 2rem;
            margin: 2rem 0;
        }

        /* Card styling */
        .card {
            background-color: var(--white);
            border-radius: 1rem;
            padding: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 4px solid;
            display: flex; /* Enable flex column */
            flex-direction: column;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }
        .card.pink { border-color: var(--soft-pink); }
        .card.purple { border-color: var(--lavender); }
        .card.green { border-color: var(--mint-green); }
        .card.blue {
            background: linear-gradient(to right, #EFF6FF, #DBEAFE); /* blue-50 to blue-100 */
            border-left: 4px solid #3B82F6; /* blue-500 */
        }
        .card h3 { margin-top: 0; font-size: 1.5rem; }
        .card p, .card ul { flex-grow: 1; } /* Allow text/list to fill space */
        .card .mt-2 { margin-top: 0.5rem; } /* Adjust spacing */


        /* Code block styling */
        .code-block {
            margin: 1.5rem 0;
            border-radius: 0.5rem;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            background-color: var(--light-gray-bg); /* Add background color */
        }
        .code-header {
            background-color: var(--dark-gray);
            color: var(--white);
            padding: 0.5rem 1rem;
            font-family: monospace;
            font-size: 0.9rem;
            border-radius: 0.5rem 0.5rem 0 0; /* Match top radius */
        }
        /* Prism overrides */
        pre[class*="language-"] {
            margin: 0 !important;
            padding: 1.5rem !important; /* More padding */
            border-radius: 0 0 0.5rem 0.5rem !important; /* Match bottom radius */
            background-color: transparent !important; /* Make transparent, parent has bg */
            font-size: 0.9rem !important;
            line-height: 1.5 !important;
            max-height: 40vh; /* Limit height and allow scroll */
            overflow: auto !important;
        }
        code[class*="language-"] {
            font-family: 'Fira Code', monospace !important;
            color: var(--dark-gray) !important; /* Adjust text color for light theme */
            background: none !important; /* Ensure no background on code element */
            text-shadow: none !important; /* Remove text shadow if any */
        }
        /* Adjust Prism theme colors slightly if needed */
        .token.comment, .token.prolog, .token.doctype, .token.cdata { color: slategray; }
        .token.punctuation { color: #999; }
        .token.property, .token.tag, .token.boolean, .token.number, .token.constant, .token.symbol, .token.deleted { color: #990055; }
        .token.selector, .token.attr-name, .token.string, .token.char, .token.builtin, .token.inserted { color: #669900; }
        .token.operator, .token.entity, .token.url, .language-css .token.string, .style .token.string { color: #a67f59; }
        .token.atrule, .token.attr-value, .token.keyword { color: #0077cc; }
        .token.function, .token.class-name { color: #DD4A68; }
        .token.regex, .token.important, .token.variable { color: #e90; }

        /* Image container */
        .image-container {
            width: 100%;
            margin: 2rem 0;
            border-radius: 1rem;
            overflow: hidden;
            /* box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08); */ /* Optional: Removed shadow for cleaner look sometimes */
            display: flex; /* Center image */
            flex-direction: column; /* Stack image and caption */
            justify-content: center;
            align-items: center;
            background-color: transparent; /* Removed BG for placeholders, let parent handle */
            text-align: center; /* Center caption */
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            max-height: 50vh; /* Limit image height */
            display: block;
            object-fit: contain; /* Ensure image fits nicely */
            border-radius: 0.5rem; /* Softer corners */
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1); /* Add shadow directly to image */
        }
        .image-caption {
            font-size: 0.85rem;
            color: var(--medium-gray);
            margin-top: 0.75rem;
        }


        /* Placeholder Box Styling */
        .placeholder-box {
            border: 2px dashed var(--lavender);
            padding: 2rem;
            text-align: center;
            color: var(--medium-gray);
            border-radius: 0.5rem;
            background-color: rgba(224, 214, 255, 0.1);
            min-height: 200px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            margin: 2rem 0;
        }

        /* Blueprint diagram styling */
        .blueprint-diagram {
             display: flex;
             flex-direction: column;
             align-items: center;
             gap: 1rem;
             margin: 2rem 0;
        }
        .blueprint-row {
            display: flex;
            justify-content: center;
            gap: 1rem;
            flex-wrap: wrap; /* Allow wrapping on smaller screens */
        }
        .blueprint-item {
            background-color: var(--white);
            border-radius: 0.5rem;
            padding: 1rem 1.5rem;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
            text-align: center;
            min-width: 120px;
            border: 1px solid #eee;
        }
        .blueprint-item strong {
            display: block;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }
        .blueprint-arrow {
            font-size: 1.5rem;
            color: var(--medium-gray);
            align-self: center;
            margin: 0 0.5rem;
        }
        .bp-input { border-left: 4px solid var(--soft-pink); }
        .bp-context { border-left: 4px solid var(--soft-pink); }
        .bp-grounding { border-left: 4px solid var(--lavender); }
        .bp-core { border-left: 4px solid var(--lavender); background-color: rgba(224, 214, 255, 0.1); }
        .bp-output { border-left: 4px solid var(--mint-green); }
        .bp-interface { border-left: 4px solid var(--mint-green); }


        /* Specific slide layouts */
        /* Title slide adjustments */
        .title-slide {
             /* background-color: var(--light-gray-bg); Gradient can be distracting */
        }
        .title-slide .slide-content {
            text-align: center;
             display: flex;
             flex-direction: column;
             justify-content: center; /* Center vertically */
             align-items: center; /* Center horizontally */
             height: 100%; /* Ensure content takes full height */
             padding-top: 0; /* Adjust padding */
             padding-bottom: 60px; /* Space for credit */
        }
         .title-slide h1 {
            color: var(--dark-gray); /* Changed title back to dark gray */
            margin-bottom: 1.5rem; /* Space between title and image */
            font-size: 4rem !important; /* Slightly larger title */
        }
         .title-slide .image-container {
             margin-top: 1rem;
             margin-bottom: 2rem;
             max-width: 60%; /* Control image size */
         }
         .title-slide .image-container img {
             max-height: 45vh; /* Adjust size as needed */
             box-shadow: none; /* Remove shadow for title image */
             border-radius: 0.5rem;
         }
        .title-slide h3 { /* Subtitle on title slide */
            font-size: clamp(1.2rem, 3vw, 1.8rem);
            font-weight: 500;
            color: var(--dark-gray);
            margin-bottom: 1.5rem;
        }
        .title-slide .affiliation {
            font-size: clamp(1.1rem, 3vw, 1.5rem);
            color: var(--dark-gray);
            margin-top: 1rem;
            margin-bottom: 1.5rem;
        }
        .title-slide .conference-info {
             font-size: clamp(0.9rem, 2vw, 1.1rem);
             color: var(--medium-gray);
             margin-top: 2rem;
        }
        .title-slide .schedule-link {
             display: inline-block;
             padding: 0.75rem 1.5rem;
             background: linear-gradient(90deg, var(--soft-pink), var(--lavender));
             color: var(--black);
             text-decoration: none;
             border-radius: 99px; /* Pill shape */
             font-weight: 600;
             margin-top: 1rem;
             transition: transform 0.2s ease, box-shadow 0.2s ease;
             box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
         }
        .title-slide .schedule-link:hover {
             transform: translateY(-3px);
             box-shadow: 0 6px 15px rgba(0, 0, 0, 0.15);
         }
        /* Fixed slide credit */
         .slide-credit {
             position: absolute;
             bottom: 20px; /* Position from bottom */
             left: 50%;
             transform: translateX(-50%);
             font-size: 0.9rem;
             color: var(--medium-gray); /* CHANGED: Visible color */
             font-weight: normal; /* Regular weight */
             z-index: 10;
             text-align: center;
             width: 90%;
         }


        .two-column-layout {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); /* Responsive two columns */
            gap: 3rem;
            align-items: center; /* Vertically align items in columns */
            margin-top: 2rem;
            width: 100%; /* Ensure it takes full width */
        }

        /* Layout for content next to an image */
        .content-with-image {
            display: grid; /* Use grid for more control */
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); /* Responsive columns */
            align-items: center; /* Vertically align items */
            gap: 2rem; /* Space between text and image */
            width: 100%;
            margin-top: 1rem;
        }
        .content-with-image .text-content {
            /* flex: 1; Takes up remaining space - Grid handles this */
        }
        .content-with-image .image-wrapper {
            /* flex-basis: 40%; Adjust width as needed - Grid handles this */
            text-align: center;
            display: flex; /* Center image within the wrapper */
            justify-content: center;
            align-items: center;
        }
        .content-with-image .image-wrapper img {
            max-width: 100%;
            height: auto;
            max-height: 60vh; /* Limit height */
            border-radius: 0.5rem;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            object-fit: contain;
        }


        /* References slide styling */
        .references-slide .slide-content {
            max-height: 90vh; /* Allow more height */
            overflow: hidden; /* Prevent content overflow */
            display: flex;
            flex-direction: column;
        }
        .references-slide h2 {
             margin-bottom: 1.5rem;
        }
        .references-list {
            flex-grow: 1; /* Allow list to take remaining space */
            overflow-y: auto; /* Enable scrolling within the list */
            font-size: 0.8rem;
            line-height: 1.5;
            column-count: 2; /* Display references in two columns */
            column-gap: 2.5rem; /* More gap */
            padding-right: 1rem; /* Padding for scrollbar */
        }
        .references-list p, .references-list ol, .references-list ul {
            margin: 0;
            padding: 0;
            list-style-position: inside; /* Keep numbers inside */
        }
        .references-list li {
            margin-bottom: 0.75rem;
            word-break: break-word;
            text-indent: -1.5em; /* Hanging indent */
            padding-left: 1.5em; /* Space for hanging indent */
        }
         .references-list .italic { font-style: italic; color: var(--medium-gray); }

        /* Citation styling */
        .citation {
            font-size: 0.8rem;
            color: var(--medium-gray);
            margin-top: 1.5rem;
            text-align: right;
            width: 100%;
        }

        /* Navigation Buttons */
        #navigation {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            z-index: 100;
            display: flex;
            gap: 1rem;
        }
        .nav-button {
            background: linear-gradient(to right, var(--soft-pink), var(--lavender), var(--mint-green));
            color: var(--black);
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 9999px; /* Pill shape */
            cursor: pointer;
            font-weight: 600;
            transition: transform 0.2s ease, box-shadow 0.2s ease, opacity 0.2s ease;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            font-size: 1rem;
        }
        .nav-button:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 15px rgba(0, 0, 0, 0.15);
        }
        .nav-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
         /* Slide Counter (Optional but helpful) */
        #slideCounter {
            position: fixed;
            bottom: 2.5rem; /* Align with buttons */
            left: 50%;
            transform: translateX(-50%);
            z-index: 100;
            font-size: 0.9rem;
            color: var(--medium-gray);
            background-color: rgba(255, 255, 255, 0.8);
            padding: 0.25rem 0.75rem;
            border-radius: 99px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            transition: opacity 0.3s ease;
            opacity: 0; /* Hidden by default */
        }
         body:hover #slideCounter {
            opacity: 1; /* Show on hover */
         }

        /* Fullscreen Image Slide Style */
        .fullscreen-image {
            background-size: contain; /* Use contain to ensure the whole image fits */
            background-position: center center;
            background-repeat: no-repeat;
            padding: 0; /* Remove padding */
            justify-content: flex-end; /* Align title towards bottom */
            align-items: center; /* Center title horizontally */
        }
        .fullscreen-image .slide-content {
             /* Override default padding/alignment if needed */
             max-width: 90%; /* Allow content to be wider */
             text-align: center;
             padding-bottom: 3rem; /* Space for the title at the bottom */
             width: auto; /* Let content determine width */
             z-index: 2; /* Ensure title is above background */
        }
        .fullscreen-image h2 {
            background-color: rgba(0, 0, 0, 0.7); /* Semi-transparent black background */
            color: white;
            padding: 0.75rem 1.5rem;
            border-radius: 8px;
            font-size: 2rem !important; /* Adjust title size */
            text-align: center;
            display: inline-block; /* Ensure background fits text */
            box-shadow: 0 2px 10px rgba(0,0,0,0.3);
            margin-bottom: 0; /* Remove default h2 margin */
        }
        .fullscreen-image h2::after {
            display: none; /* Hide underline on fullscreen image titles */
        }


        /* Responsive adjustments */
        @media (max-width: 768px) {
            h1 { font-size: 2.5rem !important; }
            h2 { font-size: 2.2rem !important; }
            h3 { font-size: clamp(1.3rem, 5vw, 1.8rem); }
            p, li { font-size: clamp(0.9rem, 4vw, 1.1rem); }
            ul { margin-left: 0.5rem; } /* Reduce list indent */

            .slide {
                padding: 3rem 5%; /* Adjust padding */
            }

            .two-column-layout, .content-with-image {
                grid-template-columns: 1fr; /* Stack columns */
                gap: 2rem;
            }
            .content-with-image .image-wrapper img {
                max-height: 40vh; /* Reduce image height on mobile */
            }
            .blueprint-diagram {
                flex-direction: column;
            }
             .blueprint-row {
                 flex-direction: column;
                 width: 100%;
                 align-items: stretch; /* Make items full width */
             }
             .blueprint-arrow {
                transform: rotate(90deg); /* Point arrows down */
                margin: 0.5rem 0;
             }
             .references-slide .references-list {
                column-count: 1; /* Single column on mobile */
                 padding-right: 0.5rem;
            }
            #navigation {
                bottom: 1rem;
                right: 1rem;
                gap: 0.5rem;
            }
            .nav-button {
                padding: 0.5rem 1rem;
                font-size: 0.9rem;
            }
            #slideCounter {
                bottom: 1.5rem;
                font-size: 0.8rem;
            }
            .fullscreen-image .slide-content { padding-bottom: 2rem; }
            .fullscreen-image h2 { font-size: 1.5rem !important; padding: 0.5rem 1rem; }
            .title-slide h1 { font-size: 2.8rem !important; }
            .title-slide .image-container { max-width: 80%; }

            /* Adjust code block font size */
            pre[class*="language-"], code[class*="language-"] {
                 font-size: 0.8rem !important;
             }
        }

    </style>
</head>
<body>
    <!-- Presentation Container -->
    <div id="presentation-container">

        <!-- Slide 0: Title -->
        <section class="slide title-slide" id="slide-0">
            <div class="slide-content">
                <h1>The Future of Multimodal AI Applications</h1>
                <div class="image-container">
                    <img src="multimodal_lego2.png" alt="Multimodal Lego Blocks">
                </div>
                 <!-- Affiliation and Conference Info can be added here if needed -->
                 <!-- Example: -->
                 <!-- <p class="affiliation">Stefania Druga</p> -->
                 <!-- <p class="conference-info">Infobip Shift Miami | May 6, 2025</p> -->
                 <!-- <a href="#" class="schedule-link">View Schedule</a> -->
            </div>
            <div class="slide-credit">Stefania Druga, Infobip Shift Miami | May 6, 2025</div>
        </section>

        <!-- Slide 1: Beyond Text -->
        <section class="slide" id="slide-1">
            <div class="slide-content">
                <h2>Beyond Text: AI That Perceives the World</h2>
                <p class="text-xl mb-6">Imagine AI that doesn't just process text, but <strong>perceives</strong> the world alongside us â€“ seeing our experiments, hearing our questions, sensing the environment.</p>
                <div class="grid-layout">
                    <div class="card pink">
                        <h3>The Limitation</h3>
                        <p>Current text-centric AI often misses the richness of real-world context and lacks direct perception.</p>
                    </div>
                    <div class="card purple">
                         <h3>The Need</h3>
                         <p>Why AI that sees, hears, senses? To build more intuitive, grounded, and truly helpful systems.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 2: Vision -->
        <section class="slide" id="slide-2">
            <div class="slide-content">
                <h2>Vision: Real-Time Multimodal AI</h2>
                <div class="content-with-image">
                    <div class="text-content">
                         <p class="text-xl mb-6"> AI systems that seamlessly integrate and synthesize information from diverse, real-time data streams:</p>
                         <ul class="list-disc list-outside space-y-1">
                             <li>Live Webcams & Video Feeds</li>
                             <li>Microphone Audio</li>
                             <li>Connected Sensors (Temperature, Location, etc.)</li>
                         </ul>
                          <p class="mt-4">Understanding context, anticipating needs, and responding dynamically through multi-sensory feedback loops.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="sensors.png" alt="Sensors Context Analysis Diagram">
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 3: Why Multimodal Matters -->
        <section class="slide gradient-bg" id="slide-3">
            <div class="slide-content">
                <h2 class="text-center">Why Multimodal AI Matters</h2>
                <div class="grid-layout grid-cols-1 md:grid-cols-3">
                    <div class="card pink">
                        <h3>Beyond Text</h3>
                        <p>Traditional AI systems rely primarily on text, limiting their ability to understand and interact with the rich, multimodal world humans naturally navigate.</p>
                    </div>
                    <div class="card purple">
                        <h3>Real-Time Interaction</h3>
                        <p>The most impactful AI systems don't just analyze - they respond dynamically to changing inputs from multiple streams with minimal latency.</p>
                    </div>
                    <div class="card green">
                        <h3>Grounding</h3>
                        <p>By connecting language to sensory inputs (vision, audio, sensors), multimodal AI anchors abstract concepts in real-world perception, leading to deeper understanding and reduced ambiguity.</p>
                    </div>
                </div>
            </div>
        </section>

         <!-- Slide 4: Intro Case Studies (Consolidated from original slides 4, 5, 6) -->
        <section class="slide center-content gradient-bg" id="slide-4">
            <div class="slide-content">
                <h2 class="text-center">Case Studies: Bringing the Blueprint to Life</h2>
                <p class="text-center text-xl mb-10">Let's explore how this blueprint applies in practice with examples from our research and industry integrations:</p>
                 <!-- Using a 2x2 grid layout -->
                 <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6 max-w-4xl">
                     <div class="card blue">
                        <h3 class="text-2xl mb-2">Gemini Smart Home</h3>
                        <p>Conversational control of smart devices directly within the Gemini AI.</p>
                    </div>
                    <div class="card pink">
                        <h3 class="text-2xl mb-2">ChemBuddy</h3>
                        <p>Making abstract chemistry tangible through real-time sensing and interaction.</p>
                    </div>
                     <div class="card purple">
                        <h3 class="text-2xl mb-2">MathMind</h3>
                        <p>Visually identifying and addressing mathematical misconceptions on the fly.</p>
                    </div>
                     <div class="card green">
                        <h3 class="text-2xl mb-2">Cognimates Copilot</h3>
                        <p>Supporting creative coding beyond text with multimodal assistance.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 5: Gemini Home Intro -->
        <section class="slide" id="slide-5">
            <div class="slide-content">
                <h2>Case Study: Gemini & Google Home Integration</h2>
                <div class="content-with-image">
                    <div class="text-content">
                        <p class="text-xl mb-4">Bringing natural language smart home control directly into the Gemini AI chat experience.</p>
                        <h3 class="mb-2 text-lg font-semibold">Core Idea:</h3>
                        <ul class="list-disc list-outside space-y-1">
                            <li>Control lights, climate, media, etc. via Gemini prompts.</li>
                            <li>Example: "Set the dining room for a romantic date night."</li>
                            <li>Reduces friction by keeping control within the AI chat context.</li>
                        </ul>
                        <p class="mt-4 text-sm text-gray-600">Project Goal: Make smart home interaction more intuitive and conversational.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="google_home_gemini_extension_2.webp" alt="Gemini Google Home Extension UI" class="max-h-[50vh] object-contain">
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 6: Gemini Home Details -->
        <section class="slide gradient-bg" id="slide-6">
            <div class="slide-content">
                <h2>Gemini Home Extension: Details & Early Access</h2>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6">
                    <div class="card"> <!-- Neutral card -->
                        <h3 class="mb-2">Capabilities & Limitations:</h3>
                        <ul class="list-disc list-outside space-y-1 mb-4">
                            <li><strong>Controls:</strong> Lighting, climate, window coverings, TVs, speakers, etc.</li>
                            <li><strong>Requires Home App for:</strong> Security devices (cameras, locks), Routines.</li>
                            <li><strong>Activation:</strong> May need '@Google Home' in prompts initially.</li>
                            <li><strong>Context:</strong> Part of broader industry trend (cf. Alexa, Siri AI upgrades).</li>
                        </ul>
                    </div>
                    <div class="card blue"> <!-- Action card -->
                        <h3 class="mb-3 text-lg font-semibold">Sign Up for Public Preview:</h3>
                        <p class="mb-4">Get early access to this feature (Android, English only initially) via the Google Home Public Preview program.</p>
                        <a href="https://support.google.com/googlenest/answer/12494697" target="_blank" class="inline-block bg-blue-600 text-white px-4 py-2 rounded hover:bg-blue-700 transition-colors">
                            Join Google Home Public Preview
                        </a>
                         <p class="text-xs mt-3 text-gray-500">Requires signing into Gemini with the same account as Google Home.</p>
                    </div>
                 </div>
            </div>
        </section>

        <!-- Slide 7: ChemBuddy Intro -->
        <section class="slide gradient-bg" id="slide-7">
             <div class="slide-content">
                <h2>Case Study 1: ChemBuddy</h2>
                <h3 class="text-lg text-gray-600 mb-6 -mt-4">AI-Powered Chemistry Lab Assistant</h3>
                <div class="content-with-image">
                    <div class="text-content">
                        <p class="text-xl mb-4">Making abstract chemistry tangible through real-time sensing and interaction.</p>
                        <h3 class="mb-2 text-lg font-semibold">Core Features:</h3>
                        <ul class="list-disc list-outside space-y-1 mb-4 text-lg">
                            <li>Real-world pH sensing via Jacdac</li>
                            <li>AI analyzes sensor data & user actions</li>
                            <li>Adaptive guidance based on experiment state</li>
                        </ul>
                        <p class="mt-4 text-md text-gray-600">Goal: Bridge concrete actions with abstract concepts.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="overview_chembuddy.png" alt="ChemBuddy Overview" class="max-h-[60vh] object-contain rounded-lg shadow-md">
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 8: ChemBuddy Experiments -->
        <section class="slide fullscreen-image" id="slide-8" style="background-image: url('chembuddy_experiments%202.png');">
            <div class="slide-content">
                 <h2>ChemBuddy Experiments Setup</h2>
            </div>
        </section>

        <!-- Slide 9: ChemBuddy Grounding -->
        <section class="slide fullscreen-image" id="slide-9" style="background-image: url('chembuddy_grounding_example%202.png');">
            <div class="slide-content">
                 <h2>ChemBuddy Grounding Example</h2>
            </div>
        </section>

        <!-- Slide 10: ChemBuddy Architecture -->
        <section class="slide" id="slide-10">
            <div class="slide-content center-content">
                <h2 class="text-center">ChemBuddy: Architecture</h2>
                <div class="image-container">
                    <img src="chembuddy_architecture.png" alt="ChemBuddy Architecture Diagram" style="max-height: 70vh;">
                    <p class="image-caption">Integrating sensors, user interface, and AI reasoning.</p>
                </div>
            </div>
        </section>

        <!-- Slide 11: MathMind Intro -->
        <section class="slide" id="slide-11">
            <div class="slide-content">
                <h2>Case Study 2: MathMind</h2>
                <h3 class="text-lg text-gray-600 mb-6 -mt-4">Visually Identifying Math Misconceptions</h3>
                 <div class="content-with-image">
                     <div class="text-content">
                         <p class="text-xl mb-4">Visually identifying and addressing mathematical misconceptions on the fly.</p>
                         <h3 class="mb-2 text-lg font-semibold">Core Features:</h3>
                         <ul class="list-disc list-outside space-y-1">
                             <li>Real-time vision analysis of handwritten work</li>
                             <li>Classification of errors against misconception taxonomy</li>
                             <li>Targeted, multimodal feedback (visual hints, explanations)</li>
                         </ul>
                         <p class="mt-4 text-sm text-gray-600">Goal: Provide timely, personalized scaffolding for math learning.</p>
                     </div>
                     <div class="image-wrapper">
                         <img src="mathmind_with_misconception.png" alt="MathMind Misconception Example" class="rounded-lg shadow-lg object-contain max-h-[70vh]">
                     </div>
                </div>
            </div>
        </section>

        <!-- Slide 12: Math Misconceptions -->
        <section class="slide fullscreen-image" id="slide-12" style="background-image: url('math_misconceptions.png');">
            <div class="slide-content">
                 <h2>Identifying Math Misconceptions</h2>
            </div>
        </section>

        <!-- Slide 13: MathMind Architecture -->
        <section class="slide" id="slide-13">
             <div class="slide-content center-content">
                 <h2 class="text-center">MathMind: Architecture</h2>
                 <div class="image-container">
                    <img src="mathmind_architecture.png" alt="MathMind Architecture Diagram" style="max-height: 70vh;">
                    <p class="image-caption">Vision input, misconception analysis, and feedback generation loop.</p>
                </div>
            </div>
        </section>

        <!-- Slide 14: Cognimates Intro & Code -->
         <section class="slide" id="slide-14">
            <div class="slide-content">
                <h2>Case Study 3: Cognimates Copilot</h2>
                 <h3 class="text-lg text-gray-600 mb-6 -mt-4">Multimodal AI Assistant for Creative Coding</h3>
                 <strong class="text-xl block mb-2 text-black">Implementation Highlight: AI Call & Response Parsing</strong>
                 <p class="mb-2">User input triggers an AI call; the response (text, code, or image data) is parsed and integrated back into the coding environment (like Scratch).</p>
                <div class="code-block">
                    <div class="code-header">Cognimates: Multimodal Response Handling (Conceptual)</div>
                    <pre><code class="language-javascript">// Core logic when user asks for help or an asset
async function handleUserInput(prompt, context) {
  // --> 1. Call AI Model (e.g., Gemini) with prompt and context
  const aiResponse = await callAIModelAPI(prompt, context);

  // 2. Parse response to determine type (text, code, image)
  const parsed = parseAIResponse(aiResponse);

  // 3. Integrate back into UI
  if (parsed.type === 'image_asset') {
    // --> Optional: Call background removal API/library
    const finalImage = await removeBackground(parsed.imageBase64);
    displayImageAsset(finalImage); // Add to Scratch assets
  } else if (parsed.type === 'code_suggestion') {
    displayCodeSuggestion(parsed.codeBlocks); // Show blocks in UI
  } else {
    displayExplanation(parsed.text); // Show text in chat
  }
}</code></pre>
                </div>
                 <div class="mt-6 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
                    <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
                    <p class="text-lg">Multimodal copilots can significantly lower barriers to creative expression in coding by offering contextual help and asset generation directly within the workflow, moving beyond simple text-based assistance.</p>
                    <p class="mt-2 text-md text-gray-600"><strong>Ethical Touchpoint:</strong> Important to consider user agency and avoid over-reliance, ensuring the AI assists rather than dictates the creative process.</p>
                 </div>
            </div>
        </section>

        <!-- Slide 15: Cognimates Overview -->
        <section class="slide fullscreen-image" id="slide-15" style="background-image: url('copilot_overview%202.png');">
            <div class="slide-content">
                 <h2>Cognimates Copilot Overview</h2>
            </div>
        </section>

        <!-- Slide 16: Cognimates Codelab UI -->
        <section class="slide fullscreen-image" id="slide-16" style="background-image: url('codelab-ui-in-browser-v3.png');">
            <div class="slide-content">
                 <h2>Cognimates Codelab UI</h2>
            </div>
        </section>

        <!-- Slide 17: Cognimates Image Training (Initial) -->
        <section class="slide fullscreen-image" id="slide-17" style="background-image: url('cognimates_image_training_old%202.png');">
            <div class="slide-content">
                 <h2>Cognimates Image Training (Initial)</h2>
            </div>
        </section>

        <!-- Slide 18: Cognimates Image Training (Refined) -->
        <section class="slide fullscreen-image" id="slide-18" style="background-image: url('cognimates_training_new.png');">
            <div class="slide-content">
                 <h2>Cognimates Image Training (Refined)</h2>
            </div>
        </section>

        <!-- Slide 19: Cognimates Eval Mexico -->
        <section class="slide fullscreen-image" id="slide-19" style="background-image: url('copilot_eval_mexico.png');">
            <div class="slide-content">
                 <h2>Cognimates Copilot Evaluation (Mexico)</h2>
            </div>
        </section>

        <!-- Slide 20: Cognimates User Studies Summary -->
        <section class="slide fullscreen-image" id="slide-20" style="background-image: url('summary_user_studies_copilot.png');">
            <div class="slide-content">
                 <h2>Cognimates Copilot User Studies Summary</h2>
            </div>
        </section>

        <!-- Slide 21: Cognimates Copilot Architecture -->
        <section class="slide" id="slide-21">
            <div class="slide-content center-content">
                <h2 class="text-center">Cognimates Copilot: Architecture</h2>
                 <div class="image-container">
                    <img src="cognimates_copilot_architecture.png" alt="Cognimates Copilot Architecture Diagram" style="max-height: 70vh;">
                    <p class="image-caption">Connecting user interface, AI models, and coding environment.</p>
                </div>
            </div>
        </section>

        <!-- Slide 22: Cognimates New Features -->
        <section class="slide fullscreen-image" id="slide-22" style="background-image: url('copilot_new_features.png');">
            <div class="slide-content">
                 <h2>Cognimates Copilot: New Features</h2>
            </div>
        </section>

        <!-- Slide 23: Cognimates Simulations -->
        <section class="slide fullscreen-image" id="slide-23" style="background-image: url('copilot_simulations.png');">
            <div class="slide-content">
                 <h2>Cognimates Copilot: Simulations</h2>
            </div>
        </section>

        <!-- Slide 24: Latency & Synchronization (Consolidated) -->
        <section class="slide" id="slide-24">
            <div class="slide-content">
                <h2 class="text-center">Challenge: Real-Time Latency & Synchronization</h2>
                <p class="text-xl mb-6 text-center">Real-time multimodal systems require overcoming critical engineering hurdles for seamless interaction:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6 items-start">
                    <div class="card blue">
                        <h3>Minimizing Latency</h3>
                        <p class="mb-2">Techniques to reduce delay between input and response:</p>
                        <ul class="list-disc pl-5 space-y-1 text-sm">
                            <li><strong>Efficient Models:</strong> Using smaller or specialized architectures (SmolVLM, State Space Models).</li>
                            <li><strong>Optimized Inference:</strong> Frameworks like TensorRT-LLM, MLX for faster computation.</li>
                            <li><strong>Quantization:</strong> Reducing model size and speeding up processing (e.g., 4-bit, FP8).</li>
                            <li><strong>Edge Computing:</strong> Processing data closer to the source.</li>
                            <li><strong>Asynchronous Processing:</strong> Handling non-critical tasks in the background.</li>
                            <li><strong>API Optimization:</strong> Techniques like batching and caching requests.</li>
                        </ul>
                        <p class="mt-3 font-semibold">Goal: Achieve near-instantaneous response (<100-200ms) for interactive tasks.</p>
                    </div>
                    <div class="card green">
                        <h3>Ensuring Synchronization</h3>
                        <p class="mb-2">Aligning data from different streams arriving at different times:</p>
                         <ul class="list-disc pl-5 space-y-1 text-sm">
                            <li><strong>Alignment Algorithms:</strong> Time-based buffering, feature-level fusion, model-based coordination.</li>
                            <li><strong>Adaptive Temporal Mapping:</strong> Handling network jitter and variable stream delays.</li>
                            <li><strong>Cross-Modal Correlation:</strong> Identifying related events across modalities.</li>
                            <li><strong>Unified Data Platforms:</strong> Centralized handling to minimize integration sync issues.</li>
                            <li><strong>Critical Window:</strong> Research suggests ~32-45ms max delay for general coherence, potentially tighter for specific senses (e.g., tactile/visual).</li>
                        </ul>
                         <p class="mt-3 font-semibold">Goal: Maintain temporal consistency for accurate context understanding.</p>
                    </div>
                </div>
                 <div class="image-container mt-4">
                    <img src="coherence_window.png" alt="Graph showing latency spikes and synchronization windows" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                    <p class="image-caption">Visualizing the complexities of managing variable latency and ensuring data synchronization.</p>
                 </div>
                 <div class="citation">Timing Window Source: "Advances in Computer AI-assisted Multimodal Data Fusion Techniques" (2024)</div>
            </div>
        </section>

        <!-- Slide 25: Parallel Processing Architectures -->
        <section class="slide gradient-bg" id="slide-25">
            <div class="slide-content">
                <h2 class="text-center">Architectures for Parallel Processing</h2>
                <p class="text-center">Handling multiple data streams efficiently often requires parallel processing strategies:</p>
                 <div class="content-with-image">
                    <div class="text-content">
                        <ul class="list-disc pl-5 space-y-2 text-lg">
                            <li><strong>Modality-Specific Pipelines:</strong> Dedicated processing paths for vision, audio, sensors before fusion.</li>
                            <li><strong>Asynchronous Task Handling:</strong> Utilizing background tasks/queues for non-critical processing (e.g., logging, detailed analysis).</li>
                            <li><strong>Hardware Acceleration:</strong> Leveraging GPUs, TPUs, or specialized AI chips for computationally intensive tasks.</li>
                            <li><strong>Distributed Systems / Edge Computing:</strong> Processing data closer to the source to reduce central load and latency.</li>
                            <li><strong>Optimized Scheduling:</strong> Efficiently managing compute resources across parallel tasks.</li>
                        </ul>
                        <p class="mt-4 font-semibold">Goal: Maximize throughput and responsiveness by handling concurrent data streams effectively.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="parallel_processing.png" alt="Parallel Processing Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                        <p class="image-caption">Conceptual model of parallel data stream processing.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 26: Advanced Data Fusion -->
        <section class="slide" id="slide-26">
             <div class="slide-content">
                <h2 class="text-center">Advanced Data Fusion Techniques</h2>
                <p class="text-center">Integrating diverse data streams (vision, audio, sensor, text) effectively is key. Moving beyond simple concatenation to leverage techniques like:</p>
                <div class="content-with-image">
                    <!-- Text Column -->
                    <div class="text-content space-y-3">
                        <ul class="list-disc list-outside ml-5 text-lg">
                            <li><strong>Deep Learning Fusion:</strong> End-to-end models with modality-specific branches fusing at different layers.</li>
                            <li><strong>Attention Mechanisms:</strong> Cross-modal attention to dynamically weigh feature importance based on context.</li>
                            <li><strong>Transformer Variants:</strong> Models like ViLBERT, CoCa designed for powerful cross-modal interaction.</li>
                            <li><strong>Graph Neural Networks (GNNs):</strong> Modeling relationships, especially useful in structured data or robotics (e.g., scene graphs).</li>
                        </ul>
                        <p class="mt-4 font-medium text-lg"><strong>Goal:</strong> Create unified, contextually rich representations that are more informative than any single modality alone.</p>
                    </div>
                    <!-- Image Column -->
                    <div class="image-wrapper">
                        <img src="fusion.png" alt="Data Fusion Approaches Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                         <p class="image-caption">Illustrating different points and methods for data fusion.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 27: Training & Customization -->
        <section class="slide gradient-bg" id="slide-27">
            <div class="slide-content">
                <h2 class="text-center">Training & Customization Strategies</h2>
                <p class="text-center">Adapting models for specific tasks or domains often involves specialized training techniques:</p>
                <div class="content-with-image">
                    <div class="text-content">
                        <ul class="list-disc pl-5 space-y-2 text-lg">
                            <li><strong>Fine-tuning:</strong> Adapting pre-trained foundation models (e.g., VLMs) on smaller, domain-specific datasets.</li>
                            <li><strong>Parameter-Efficient Fine-Tuning (PEFT):</strong> Methods like LoRA, QLoRA to adapt large models efficiently with fewer trainable parameters.</li>
                            <li><strong>Synthetic Data Generation:</strong> Creating artificial data (images, sensor readings, text interactions) to augment limited real-world data, crucial for rare events or specific scenarios.</li>
                            <li><strong>Few-Shot / Zero-Shot Learning:</strong> Enabling models to perform tasks with minimal or no specific training examples by leveraging their pre-trained knowledge.</li>
                            <li><strong>Custom Sensor Models:</strong> Training smaller, specialized models directly on specific sensor inputs (e.g., IMU for activity recognition) for edge efficiency.</li>
                        </ul>
                        <p class="mt-4 font-semibold">Goal: Improve performance on target tasks, handle data scarcity, and enable deployment across diverse hardware.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="cognimates_training_new.png" alt="Cognimates Model Training Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                         <p class="image-caption">Conceptual example of training pipelines, potentially involving fine-tuning and synthetic data.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 28: Evaluation - General Benchmarks -->
         <section class="slide title-slide gradient-bg" id="slide-28"> <!-- Using title-slide style for centering -->
            <div class="slide-content">
                <h2 class="text-center">Evaluating Multimodal Systems: General Benchmarks</h2>
                <p class="text-xl mb-6 text-center">Assessing the general capabilities and reliability of multimodal AI requires diverse evaluation methods:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6 max-w-5xl">
                    <div class="card pink">
                        <h3>Key General Benchmarks</h3>
                        <ul class="list-disc pl-5 space-y-1 text-sm">
                            <li><strong>MMMU:</strong> Broad university-level multimodal understanding.</li>
                            <li><strong>MathVista:</strong> Visual mathematical reasoning.</li>
                            <li><strong>MMBench / MME / SEED-Bench:</strong> General visual perception and understanding.</li>
                            <li><strong>MM-Vet:</strong> Assessing core VLM capabilities.</li>
                            <li><strong>DocVQA / TextVQA:</strong> Document and text-based visual question answering.</li>
                            <li><strong>Video-MME / CinePile:</strong> Video understanding benchmarks.</li>
                            <li><strong>Audio/Speech:</strong> FLEURS, VoxCeleb, LibriSpeech (for ASR components).</li>
                        </ul>
                        <p class="mt-2">Evaluating capabilities across diverse tasks and modalities.</p>
                    </div>
                    <div class="card purple">
                        <h3>General Testing Strategies</h3>
                         <ul class="list-disc pl-5 space-y-1 text-sm">
                            <li><strong>Component Testing:</strong> Evaluating individual modules (vision, audio processing, fusion).</li>
                            <li><strong>End-to-End Testing:</strong> Assessing the entire system performance on integrated tasks.</li>
                             <li><strong>Robustness Testing:</strong> Evaluating performance under noise, adversarial attacks, or missing data.</li>
                            <li><strong>Bias and Fairness Audits:</strong> Checking for demographic or subgroup performance disparities.</li>
                            <li><strong>Efficiency Metrics:</strong> Measuring latency, throughput, resource consumption (memory, CPU/GPU).</li>
                             <li><strong>Monitoring in Deployment:</strong> Tracking real-world performance and collecting feedback.</li>
                        </ul>
                         <p class="mt-2">Ensuring reliability and performance beyond standard accuracy metrics.</p>
                    </div>
                </div>
                 <div class="image-container grid grid-cols-2 gap-4 mt-6 max-w-4xl">
                     <img src="evaluation.png" alt="Evaluation Metrics Concept" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                     <p class="image-caption col-span-1 self-center text-sm">Diverse metrics are needed.</p>
                     <img src="latency_sync.png" alt="Latency / Throughput Concept" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                     <p class="image-caption col-span-1 self-center text-sm">Efficiency is crucial.</p>
                 </div>
            </div>
        </section>

        <!-- Slide 29: Domain-Specific Evaluation -->
        <section class="slide" id="slide-29">
            <div class="slide-content">
                <h2>Beyond General Benchmarks: Domain-Specific Evaluation</h2>
                <p class="text-xl mb-6">Truly understanding co-scientist or assistant performance requires evaluation tailored to specific application areas and user goals.</p>
                <div class="content-with-image">
                    <div>
                        <strong class="text-xl block mb-2 text-black">Why Domain-Specific?</strong>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Captures nuances missed by broad tests (e.g., specific scientific concepts, coding patterns).</li>
                            <li>Aligns evaluation with real-world tasks and workflows.</li>
                            <li>Allows measurement of context-specific understanding and reasoning.</li>
                            <li>Provides more actionable insights for improvement in the target domain.</li>
                            <li>Essential for safety-critical applications (healthcare, robotics).</li>
                        </ul>
                        <strong class="text-xl block mt-4 mb-2 text-black">Example: Math & Science Education</strong>
                        <p>Developing benchmarks specifically focused on identifying and addressing common mathematical or scientific misconceptions demonstrated by students during problem-solving.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="eval_mathmind%202.png" alt="MathMind Evaluation Example showing student work analysis" class="rounded-lg shadow-lg object-contain max-h-[55vh]">
                        <p class="image-caption">Evaluating misconception detection in student's work.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 30: Creative Coding Evaluation -->
        <section class="slide gradient-bg" id="slide-30">
            <div class="slide-content">
                <h2>Evaluating Creative Coding Copilots</h2>
                <p class="text-xl mb-6">Assessing AI assistance in open-ended creative tasks like coding requires different approaches than traditional benchmarks.</p>
                <div class="content-with-image">
                     <div>
                        <strong class="text-xl block mb-2 text-black">Challenges & Approaches:</strong>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Defining "success" or "correctness" in creative tasks is subjective and multi-faceted.</li>
                            <li>Need to evaluate the *process* (e.g., exploration, overcoming blocks) as much as the final *product*.</li>
                            <li>Measuring impact on user learning, engagement, self-efficacy, and ability to express ideas.</li>
                            <li>Developing benchmarks that simulate real coding scenarios (e.g., completing a partial project, debugging visual errors, generating specific types of assets/interactions).</li>
                            <li>Incorporating qualitative user feedback heavily.</li>
                        </ul>
                        <strong class="text-xl block mt-4 mb-2 text-black">Example: Cognimates Copilot Evaluation</strong>
                        <p>Using project-based scenarios and user studies to evaluate the copilot's ability to provide relevant code suggestions, explain programming concepts clearly, and generate useful visual assets within the Scratch/block-based environment.</p>
                    </div>
                    <div class="image-wrapper">
                        <img src="copilot_evaluation_benchmark.png" alt="Cognimates Copilot Evaluation Benchmark Example" class="rounded-lg shadow-lg object-contain max-h-[55vh]">
                         <p class="image-caption">Task-based evaluation scenario for a coding copilot.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 31: AI Tutor Evaluation Framework -->
        <section class="slide" id="slide-31">
            <div class="slide-content">
                <h2 class="text-center">Standardizing AI Tutor Evaluation: A Pedagogical Framework</h2>
                <p class="text-lg mb-4 text-center">The <a href="https://github.com/kaushal0494/UnifyingAITutorEvaluation" target="_blank" class="text-purple-600 hover:underline">Unifying AI Tutor Evaluation</a> framework proposes a taxonomy to assess the pedagogical abilities of LLM-based tutors across key dimensions.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
                    <!-- Left Column: Image and Description -->
                    <div class="space-y-4">
                         <p>This structured approach uses a detailed JSON format to capture tutor responses and annotate them across dimensions like mistake identification, guidance quality, coherence, revealing answers, and tone. It aims to provide a consistent method for comparing different AI tutoring systems.</p>
                         <div class="image-container mt-4">
                            <img src="evaluation.png" alt="Diagram illustrating the AI Tutor Evaluation Framework components" class="rounded-lg shadow-lg object-contain max-h-[45vh]">
                            <p class="image-caption">Components of the pedagogical evaluation framework.</p>
                         </div>
                    </div>
                    <!-- Right Column: Simplified JSON Structure -->
                    <div class="code-block max-h-[65vh] overflow-y-auto">
                        <div class="code-header">Evaluation Structure (Simplified JSON)</div>
                        <pre><code class="language-json">{
  "conversation_id": "...",
  "task_description": "...",
  "student_attempt": "...",
  "ground_truth_solution": "...",
  "model_responses": {
    "Tutor_Model_A": {
      "response": "Tutor's reply to student...",
      "annotations": {
        "Mistake_Identification": "Correct/Incorrect/Partial",
        "Mistake_Location": "Precise/Vague/None",
        "Revealing_Answer": "Yes/Hint/No",
        "Providing_Guidance": "Specific/Generic/None",
        "Guidance_Quality": "Helpful/Confusing/Irrelevant",
        "Actionability": "Clear Next Step/Unclear",
        "Coherence": "Coherent/Incoherent",
        "Tutor_Tone": "Encouraging/Neutral/Discouraging",
        "Humanlikeness": "High/Medium/Low"
        // ... other dimensions
      }
    }
    // ... responses from other models
  }
}</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 32: User Research & Testing -->
        <section class="slide gradient-bg" id="slide-32">
            <div class="slide-content">
                <h2 class="text-center">User Research & Testing in Multimodal AI</h2>
                 <p class="text-xl mb-6 text-center">Understanding user interaction and experience is critical, especially for assistants, copilots, and Human-Robot Interaction (HRI):</p>
                <div class="content-with-image">
                    <!-- Text Column -->
                    <div class="text-content space-y-3">
                        <ul class="list-disc list-outside ml-5 text-lg">
                            <li><strong>Observational Studies:</strong> Analyzing how users interact naturally with the system in realistic settings (Wizard-of-Oz, field studies).</li>
                            <li><strong>Task-Based Evaluations:</strong> Measuring success rates, task completion time, efficiency, and errors on specific, representative tasks.</li>
                            <li><strong>Qualitative Feedback:</strong> Think-aloud protocols, interviews, surveys (SUS, UMUX) to capture user perception, satisfaction, mental models, and pain points.</li>
                            <li><strong>Analyzing Interaction Cues:</strong> Using AI or human observation to understand user state (engagement, confusion, frustration) from non-verbal cues (gaze, posture) during interaction.</li>
                            <li><strong>Iterative Design & Prototyping:</strong> Incorporating feedback rapidly into development cycles through low and high-fidelity prototypes.</li>
                        </ul>
                        <p class="mt-4 font-medium text-lg"><strong>Goal:</strong> Build systems that are not just capable, but also intuitive, effective, trustworthy, and meet genuine user needs in real-world contexts.</p>
                    </div>
                    <!-- Image Column -->
                    <div class="image-wrapper">
                        <img src="user_testing.png" alt="User Research and Testing Cycle Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                         <p class="image-caption">Iterative user-centered design cycle.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 33: Core Patterns - Input Processing -->
        <section class="slide gradient-bg" id="slide-33">
             <div class="slide-content center-content">
                <h2 class="text-center">Core Pattern: Input Processing Architectures</h2>
                 <p class="text-center mb-6">How incoming data streams are handled before fusion.</p>
                <div class="image-container mb-4">
                    <img src="chembuddy_architecture.png" alt="Multimodal Input Processing Architectures" style="max-height: 250px;">
                    <p class="image-caption">Conceptualizing input pipelines (example from ChemBuddy).</p>
                </div>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 max-w-4xl">
                    <div class="card pink">
                        <h3>Parallel Processing</h3>
                        <p>Process each modality (vision, audio, sensor) concurrently through specialized pipelines before fusion.</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>Good for diverse modalities with different processing needs.</li>
                            <li>Allows specialized optimization per stream.</li>
                            <li>Can improve throughput if resources allow.</li>
                        </ul>
                    </div>
                    <div class="card purple">
                        <h3>Hierarchical Processing</h3>
                        <p>Progressive abstraction of features across modalities, potentially fusing at multiple levels (early, mid, late).</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>Can capture low-level correlations early.</li>
                            <li>May allow cross-modal transfer learning effects.</li>
                            <li>Mirrors some aspects of biological perception.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 34: Core Patterns - Model Selection -->
        <section class="slide" id="slide-34">
            <div class="slide-content center-content">
                <h2 class="text-center">Core Pattern: Model Selection & Deployment</h2>
                 <p class="text-center mb-6">Choosing the right AI models based on capability needs and deployment constraints.</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-3 max-w-6xl">
                    <div class="card pink">
                        <h3>Cloud / API Models</h3>
                        <p>Large, high-capability models accessed via network.</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>Google Gemini Pro / Ultra</li>
                            <li>OpenAI GPT-4o / GPT-4V</li>
                            <li>Anthropic Claude 3.5 Sonnet</li>
                            <li>(+) Highest performance, broad knowledge</li>
                            <li>(-) Latency, cost, data privacy concerns</li>
                        </ul>
                    </div>
                    <div class="card purple">
                        <h3>Edge-Optimized Models</h3>
                        <p>Compact models designed for on-device execution.</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>Google PaliGemma / Gemma</li>
                            <li>Apple Ferret / OpenELM</li>
                            <li>Llama 3 V / SmolVLM / Moondream / Phi-3 Vision</li>
                            <li>(+) Low latency, privacy, offline capability</li>
                            <li>(-) Lower capability, limited context</li>
                        </ul>
                    </div>
                    <div class="card green">
                        <h3>Specialized Architectures</h3>
                        <p>Models with unique designs for specific tasks or efficiencies.</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>State Space Models (e.g., Mamba): Potential for low latency.</li>
                            <li>Mixture-of-Experts (MoE): Efficient scaling.</li>
                            <li>Whisper / SeamlessM4T: Speech recognition/translation.</li>
                            <li>CLIP / SigLIP: Efficient image-text embedding.</li>
                            <li>(+) Optimized for specific needs (speed, modality)</li>
                            <li>(-) May require more complex integration</li>
                        </ul>
                    </div>
                </div>
                <div class="image-container mt-4">
                    <img src="evaluation.png" alt="Model Evaluation Comparison Concept" style="max-height: 150px;">
                     <p class="image-caption">Trade-offs: capability vs. efficiency vs. specialization.</p>
                </div>
            </div>
        </section>

        <!-- Slide 35: Core Patterns - Fusion & Context -->
        <section class="slide gradient-bg" id="slide-35">
            <div class="slide-content center-content">
                <h2 class="text-center">Core Pattern: Fusion & Context Modeling</h2>
                 <p class="text-center mb-6">Techniques for integrating information across modalities and time.</p>
                <div class="image-container mb-4">
                    <img src="fusion.png" alt="Fusion Techniques Visualization" style="max-height: 250px;">
                    <p class="image-caption">Different strategies for combining multimodal data.</p>
                </div>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 max-w-4xl">
                    <div class="card pink">
                        <h3>Attention-Based Fusion</h3>
                        <p>Using transformer architectures (self-attention, cross-attention) to create dynamic relationships between modality features.</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>Allows model to weigh importance of different inputs.</li>
                            <li>State-of-the-art in many multimodal tasks.</li>
                            <li>Computationally intensive.</li>
                        </ul>
                    </div>
                    <div class="card purple">
                        <h3>Context Management</h3>
                        <p>Maintaining coherent understanding across extended interactions.</p>
                        <ul class="text-sm list-disc list-outside pl-4">
                            <li>Sliding context windows (limited memory).</li>
                            <li>Retrieval-Augmented Generation (RAG) from external knowledge.</li>
                            <li>Vector databases for semantic memory.</li>
                            <li>Hierarchical context / summarization.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 36: Future Trajectory -->
        <section class="slide gradient-bg" id="slide-36">
            <div class="slide-content">
                <h2 class="text-center">The Future Trajectory: Beyond Today's Examples</h2>
                <p class="text-center text-xl mb-10">The principles behind these examples point towards a broader future for real-time multimodal AI:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 lg:grid-cols-3">
                    <div class="card pink">
                        <h3>Personalized Assistance</h3>
                        <p>AI truly understanding user context (location, activity, physiological state via sensors) for proactive, personalized help.</p>
                    </div>
                     <div class="card purple">
                        <h3>Enhanced Accessibility</h3>
                        <p>Real-time translation between visual, audio, textual, and haptic information for diverse user needs.</p>
                    </div>
                     <div class="card green">
                        <h3>Intelligent Robotics</h3>
                        <p>Machines perceiving, understanding, and interacting naturally and safely within complex human environments.</p>
                    </div>
                     <div class="card pink">
                         <h3>Collaborative Creativity</h3>
                         <p>AI partners collaborating dynamically via sketches, gestures, voice, code, sound, and physical prototypes.</p>
                    </div>
                    <div class="card purple">
                        <h3>Adaptive Learning</h3>
                        <p>Truly personalized education responding to diverse learning styles, real-world actions, and misconceptions in real-time.</p>
                    </div>
                    <div class="card green">
                        <h3>Richer Human-AI Collaboration</h3>
                        <p>More intuitive, effective partnerships built on shared perception and understanding of the world.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Slide 37: Emerging Apps -->
        <section class="slide" id="slide-37">
            <div class="slide-content center-content">
                <h2 class="text-center">Emerging Applications: Beyond the Desktop</h2>
                <p class="text-center">Real-time multimodal AI is enabling new experiences in physical and immersive environments:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-3 gap-4 max-w-5xl">
                    <div class="card pink">
                        <h3>Extended Reality (XR/AR/VR)</h3>
                        <p>Enhanced interaction via gesture/gaze understanding, immersive training simulations, context-aware virtual agents (e.g., Meta Ray-Ban AI, Apple Vision Pro features).</p>
                    </div>
                    <div class="card purple">
                        <h3>Robotics & Embodied AI</h3>
                        <p>Improved HRI (understanding non-verbal cues), robust navigation (sensor fusion), semantic scene understanding for complex manipulation tasks.</p>
                    </div>
                     <div class="card green">
                        <h3>Smart Environments</h3>
                        <p>Intelligent transportation (dynamic routing based on traffic vision), public safety (anomaly detection), environmental monitoring using diverse sensor networks (smart cities, smart homes).</p>
                    </div>
                </div>
                 <div class="image-container mt-6">
                     <img src="google_home_gemini_extension_2.webp" alt="Google Home Gemini Extension Example" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                     <p class="image-caption">Example: Integrating powerful multimodal models into everyday devices and environments.</p>
                 </div>
                <p class="text-center mt-4">Also impacting Healthcare (Diagnostics, Monitoring), Finance (Fraud Detection), Retail (Personalization), Manufacturing (QA), and more.</p>
            </div>
        </section>

        <!-- Slide 38: Future App - Material Science -->
        <section class="slide gradient-bg center-content" id="slide-38">
            <div class="slide-content">
                <h2 class="text-center">Future Application: Accelerating Material Science</h2>
                <div class="image-container mt-4">
                    <img src="new_material_science.png" alt="AI analyzing material structures" class="rounded-lg shadow-xl object-contain max-h-[65vh] mx-auto">
                </div>
                <p class="mt-4 text-lg max-w-3xl">Multimodal AI can analyze experimental data (microscopy images, sensor readings), simulation results, and scientific literature to predict properties of novel materials and suggest new experimental paths.</p>
            </div>
        </section>

        <!-- Slide 39: Future App - CAD Design -->
        <section class="slide center-content" id="slide-39">
            <div class="slide-content">
                <h2 class="text-center">Future Application: AI-Assisted CAD & Engineering</h2>
                <div class="image-container mt-4">
                    <img src="cad_design_support.png" alt="AI assisting with CAD software" class="rounded-lg shadow-xl object-contain max-h-[65vh] mx-auto">
                </div>
                <p class="mt-4 text-lg max-w-3xl">AI agents understanding design sketches, user voice commands, engineering constraints (from text/docs), and simulation feedback to suggest optimizations, automate routine tasks, and facilitate collaborative design.</p>
            </div>
        </section>

        <!-- Slide 40: Future App - Cultural Preservation -->
        <section class="slide gradient-bg center-content" id="slide-40">
            <div class="slide-content">
                <h2 class="text-center">Future Application: Cultural Heritage Preservation</h2>
                <div class="image-container mt-4">
                    <img src="cultural_preservation.png" alt="AI helping analyze cultural artifacts" class="rounded-lg shadow-xl object-contain max-h-[65vh] mx-auto">
                </div>
                <p class="mt-4 text-lg max-w-3xl">Analyzing artifacts (3D scans, images), translating ancient texts (OCR + translation), virtually reconstructing historical sites, and making cultural knowledge accessible through multimodal interfaces.</p>
            </div>
        </section>

        <!-- Slide 41: Future App - Language Preservation -->
        <section class="slide center-content" id="slide-41">
             <div class="slide-content">
                <h2 class="text-center">Future Application: Language Documentation & Revitalization</h2>
                <div class="image-container mt-4">
                    <img src="language_preservation.png" alt="AI assisting with language documentation" class="rounded-lg shadow-xl object-contain max-h-[65vh] mx-auto">
                </div>
                <p class="mt-4 text-lg max-w-3xl">Using audio recordings, video of speakers (gestures, context), and written texts to document endangered languages, create interactive learning tools, facilitate translation, and analyze linguistic patterns.</p>
            </div>
        </section>

        <!-- Slide 42: References -->
        <section class="slide references-slide" id="slide-42">
            <div class="slide-content">
                <h2>References</h2>
                 <div class="references-list">
                    <!-- Placeholder: Add references from research_report.md here -->
                     <p class="italic">[Placeholder: Please insert the full list of references from the accompanying research_report.md document here. Format as an ordered list (ol) with list items (li) for proper display.]</p>

                     <!-- Example formatting (remove once real refs are added): -->
                     <ol class="list-decimal list-inside space-y-1" style="visibility: hidden;">
                         <li>Author A., Author B. (Year). Title of Work. Journal/Conference, Vol(Issue), pp. 1-10.</li>
                         <li>Author C. et al. (Year). Another Title. Publisher.</li>
                         <li>More Authors et al. (Year). A Third Reference Item Spanning Multiple Lines to Test Wrapping and Indentation. Conference Proceedings.</li>
                         <li>Yet Another Author. (Year). Book Title. Publisher Name.</li>
                         <li>Reference 5.</li>
                         <li>Reference 6.</li>
                         <li>Reference 7.</li>
                         <li>Reference 8.</li>
                         <li>Reference 9.</li>
                         <li>Reference 10.</li>
                         <li>Reference 11.</li>
                         <li>Reference 12.</li>
                     </ol>

                </div>
            </div>
        </section>

        <!-- Add more slides here following the pattern -->
        <!-- e.g., <section class="slide" id="slide-43"> ... </section> -->

    </div> <!-- End of presentation-container -->

    <!-- Navigation Buttons & Counter -->
    <div id="navigation">
        <button id="prevBtn" class="nav-button" onclick="prevSlide()" title="Previous Slide (Left Arrow)">â–²</button>
        <button id="nextBtn" class="nav-button" onclick="nextSlide()" title="Next Slide (Right Arrow/Space)">â–¼</button>
    </div>
    <div id="slideCounter">1 / N</div>

    <script>
        const presentationContainer = document.getElementById('presentation-container');
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        const prevButton = document.getElementById('prevBtn');
        const nextButton = document.getElementById('nextBtn');
        const slideCounter = document.getElementById('slideCounter');

        let currentSlideIndex = 0;
        let isScrolling = false; // Flag to prevent interference during programmatic scroll

        // --- Core Navigation Logic ---

        function updateNavigationState() {
            // Update Hash (use replaceState to avoid history pollution)
            const newHash = `#slide-${currentSlideIndex}`;
            if (window.location.hash !== newHash) {
                // Only update if it's different, prevents loops with observer
                 history.replaceState(null, null, newHash);
                 // console.log(`Hash updated to: ${newHash}`);
            }

            // Update Buttons
            prevButton.disabled = currentSlideIndex === 0;
            nextButton.disabled = currentSlideIndex === totalSlides - 1;

            // Update Counter
            slideCounter.textContent = `${currentSlideIndex + 1} / ${totalSlides}`;
            // console.log(`Current Index: ${currentSlideIndex}`);
        }

        function goToSlide(index, smooth = true) {
            if (index < 0 || index >= totalSlides || isScrolling) {
                return;
            }
            // console.log(`Attempting to scroll to index: ${index}`);
            const targetSlide = slides[index];
            if (targetSlide) {
                isScrolling = true;
                currentSlideIndex = index; // Update index immediately for button state
                updateNavigationState(); // Update buttons/counter right away

                targetSlide.scrollIntoView({
                    behavior: smooth ? 'smooth' : 'auto',
                    block: 'start' // Ensure top of slide aligns with top of container
                });

                 // Reset scrolling flag after a short delay to allow scroll to finish
                 // This prevents the observer from fighting the programmatic scroll
                 setTimeout(() => {
                    isScrolling = false;
                    // Double-check state after scroll, observer might have fired
                    // updateNavigationState();
                    // console.log(`Scroll complete, isScrolling reset`);
                 }, 800); // Adjust delay as needed (longer for very smooth scrolls)
            }
        }

        function nextSlide() {
            goToSlide(currentSlideIndex + 1);
        }

        function prevSlide() {
            goToSlide(currentSlideIndex - 1);
        }

        // --- Intersection Observer for Manual Scrolling ---

        const observerOptions = {
            root: presentationContainer, // Observe within the container
            rootMargin: '0px',
            threshold: 0.6 // Trigger when 60% of the slide is visible
        };

        const observerCallback = (entries, observer) => {
            entries.forEach(entry => {
                if (entry.isIntersecting && !isScrolling) {
                    // Find the index of the intersecting slide
                    const intersectingIndex = Array.from(slides).indexOf(entry.target);
                     if (intersectingIndex !== -1 && intersectingIndex !== currentSlideIndex) {
                        // console.log(`Observer detected slide ${intersectingIndex} intersecting`);
                        currentSlideIndex = intersectingIndex;
                        updateNavigationState();
                    }
                }
            });
        };

        const observer = new IntersectionObserver(observerCallback, observerOptions);

        // --- Initialization ---

        document.addEventListener('DOMContentLoaded', () => {
            // Observe all slides
            slides.forEach(slide => observer.observe(slide));

            // Initial state setup based on hash or default to 0
            const initialHash = window.location.hash;
            let targetIndex = 0;
            if (initialHash && initialHash.startsWith('#slide-')) {
                const indexFromHash = parseInt(initialHash.substring(7)); // Length of '#slide-'
                if (!isNaN(indexFromHash) && indexFromHash >= 0 && indexFromHash < totalSlides) {
                    targetIndex = indexFromHash;
                    // console.log(`Loading initial slide from hash: ${targetIndex}`);
                }
            }

            // Need a slight delay for layout to settle before scrolling on load
            setTimeout(() => {
                goToSlide(targetIndex, false); // Go to initial slide without smooth scroll
                // Manually update state once after initial jump
                 currentSlideIndex = targetIndex;
                 updateNavigationState();
            }, 100);


            // Keyboard listeners
            document.addEventListener('keydown', (event) => {
                 // Prevent scrolling interfering with code blocks or text areas
                 if (document.activeElement.tagName === 'TEXTAREA' || document.activeElement.tagName === 'PRE' || document.activeElement.isContentEditable) {
                     return;
                 }

                if (event.key === 'ArrowRight' || event.key === ' ' || event.key === 'PageDown') {
                    event.preventDefault(); // Prevent default space bar scroll
                    nextSlide();
                } else if (event.key === 'ArrowLeft' || event.key === 'PageUp') {
                    event.preventDefault();
                    prevSlide();
                } else if (event.key === 'Home') {
                     event.preventDefault();
                     goToSlide(0);
                 } else if (event.key === 'End') {
                     event.preventDefault();
                     goToSlide(totalSlides - 1);
                 }
            });

             // Optional: Re-evaluate current slide if window is resized (snap points might change)
             let resizeTimeout;
             window.addEventListener('resize', () => {
                 clearTimeout(resizeTimeout);
                 resizeTimeout = setTimeout(() => {
                     // Force a scroll check after resize settles
                     const currentScroll = presentationContainer.scrollTop;
                     const containerHeight = presentationContainer.clientHeight;
                     const mostVisibleIndex = Math.round(currentScroll / containerHeight);
                     if (mostVisibleIndex !== currentSlideIndex && !isScrolling) {
                        // console.log("Adjusting index after resize");
                        currentSlideIndex = mostVisibleIndex;
                        updateNavigationState();
                     }
                 }, 250);
             });
        });

    </script>
</body>
</html>