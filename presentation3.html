<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Future of Multimodal AI Applications</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-typescript.min.js"></script>
    <style>
        :root {
            --black: #000000;
            --white: #FFFFFF;
            --soft-pink: #FFD6E0;
            --lavender: #E0D6FF;
            --mint-green: #C1F0DB;
            --dark-gray: #333333;
            --medium-gray: #666666;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }

        body {
            background-color: var(--white);
            color: var(--black);
            line-height: 1.6;
        }

        .slide {
            width: 100%;
            min-height: 100vh;
            padding: 2rem;
            display: flex;
            flex-direction: column;
            justify-content: center;
            scroll-snap-align: start;
            position: relative;
            overflow: hidden;
        }

        .slide-content {
            max-width: 1200px;
            margin: 0 auto;
            width: 100%;
            z-index: 1;
        }

        .gradient-bg {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, rgba(255, 214, 224, 0.1), rgba(224, 214, 255, 0.1), rgba(193, 240, 219, 0.1));
            z-index: 0;
        }

        h1, h2, h3, h4, h5, h6 {
            margin-bottom: 1rem;
            font-weight: 700;
        }

        h1 {
            font-size: 3.5rem;
            background: linear-gradient(90deg, var(--soft-pink), var(--lavender), var(--mint-green));
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            display: inline-block;
        }

        h2 {
            font-size: 2.5rem;
            color: var(--black);
        }

        h3 {
            font-size: 1.8rem;
            color: var(--dark-gray);
        }

        p, li {
            margin-bottom: 1rem;
            color: var(--dark-gray);
            font-size: 1.2rem;
        }

        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .card {
            background-color: var(--white);
            border-radius: 1rem;
            padding: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 4px solid;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }

        .card.pink {
            border-color: var(--soft-pink);
        }

        .card.purple {
            border-color: var(--lavender);
        }

        .card.green {
            border-color: var(--mint-green);
        }

        .card h3 {
            margin-top: 0;
        }

        .code-block {
            margin: 1.5rem 0;
            border-radius: 0.5rem;
            overflow: hidden;
        }

        .code-header {
            background-color: var(--dark-gray);
            color: var(--white);
            padding: 0.5rem 1rem;
            font-family: monospace;
            font-size: 0.9rem;
        }

        pre {
            margin: 0 !important;
            padding: 1rem !important;
            border-radius: 0 0 0.5rem 0.5rem !important;
            background-color: #f7f7f7 !important;
        }

        code {
            font-family: 'Fira Code', monospace !important;
            font-size: 0.9rem !important;
        }

        .image-container {
            width: 100%;
            margin: 2rem 0;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
        }

        .image-container img {
            width: 100%;
            height: auto;
            display: block;
        }

        .blueprint {
            background-color: #f7f7f7;
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .blueprint-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1.5rem;
        }

        .blueprint-item {
            background-color: var(--white);
            border-radius: 0.5rem;
            padding: 1.5rem;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
        }

        .blueprint-item h4 {
            margin-top: 0;
            color: var(--black);
            font-size: 1.2rem;
        }

        .gradient-text {
            background: linear-gradient(90deg, var(--soft-pink), var(--lavender), var(--mint-green));
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            font-weight: 700;
        }

        .app-showcase {
            display: flex;
            flex-wrap: wrap;
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .app-item {
            flex: 1 1 300px;
            background-color: var(--white);
            border-radius: 1rem;
            padding: 1.5rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            transition: transform 0.3s ease;
        }

        .app-item:hover {
            transform: translateY(-5px);
        }

        .app-header {
            display: flex;
            align-items: center;
            margin-bottom: 1rem;
        }

        .app-icon {
            width: 50px;
            height: 50px;
            border-radius: 12px;
            margin-right: 1rem;
            background: linear-gradient(135deg, var(--soft-pink), var(--lavender));
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            color: var(--white);
            font-size: 1.2rem;
        }

        .citation {
            font-size: 0.8rem;
            color: var(--medium-gray);
            margin-top: 0.5rem;
        }

        .section-header {
            position: relative;
            margin-bottom: 2.5rem;
            padding-bottom: 0.5rem;
        }

        .section-header::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 60px;
            height: 4px;
            background: linear-gradient(90deg, var(--soft-pink), var(--lavender), var(--mint-green));
            border-radius: 2px;
        }

        .center-content {
            text-align: center;
        }

        .center-content .section-header::after {
            left: 50%;
            transform: translateX(-50%);
        }
        
        .sensor-blocks {
            display: flex;
            gap: 1.5rem;
            margin: 2rem 0;
            justify-content: center;
        }
        
        .sensor-block {
            width: 120px;
            height: 120px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--white);
            font-weight: bold;
            font-size: 1.2rem;
            border-radius: 0.5rem;
        }
        
        .sensor-camera {
            background-color: #9D4EDD;
        }
        
        .sensor-audio {
            background-color: #40A9FF;
        }
        
        .sensor-data {
            background-color: #52C41A;
        }
        
        .sensor-text {
            margin-top: 0.5rem;
            text-align: center;
            font-size: 0.9rem;
        }
        
        .conclusion-points {
            margin: 2rem 0;
        }
        
        .conclusion-point {
            display: flex;
            margin-bottom: 1.5rem;
        }
        
        .point-number {
            font-size: 2rem;
            font-weight: 700;
            margin-right: 1rem;
            color: var(--lavender);
            line-height: 1;
        }
        
        .navigation {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            z-index: 100;
            display: flex;
            gap: 0.5rem;
        }
        
        .nav-button {
            width: 50px;
            height: 50px;
            border-radius: 50%;
            background-color: var(--white);
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: transform 0.2s ease;
        }
        
        .nav-button:hover {
            transform: scale(1.1);
        }

        .schedule-link {
            display: inline-block;
            padding: 0.75rem 1.5rem;
            background: linear-gradient(90deg, var(--soft-pink), var(--lavender));
            color: var(--white);
            text-decoration: none;
            border-radius: 0.5rem;
            font-weight: 600;
            margin-top: 1rem;
            transition: transform 0.2s ease;
        }

        .schedule-link:hover {
            transform: translateY(-3px);
        }

        .sensor-diagram {
            display: flex;
            justify-content: center;
            margin: 2rem 0;
        }
        
        .multimodal-arch {
            width: 100%;
            max-width: 800px;
            margin: 2rem auto;
            background-color: var(--white);
            border-radius: 1rem;
            padding: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
        }
        
        .arch-layer {
            display: flex;
            margin-bottom: 1.5rem;
            align-items: center;
        }
        
        .layer-label {
            width: 25%;
            padding: 1rem;
            text-align: center;
            font-weight: 600;
            border-radius: 0.5rem;
        }
        
        .layer-components {
            width: 75%;
            display: flex;
            gap: 1rem;
            margin-left: 1rem;
        }
        
        .component {
            flex: 1;
            padding: 1rem;
            background-color: #f7f7f7;
            border-radius: 0.5rem;
            text-align: center;
        }
        
        .inputs-layer .layer-label {
            background-color: var(--soft-pink);
            color: var(--black);
        }
        
        .processing-layer .layer-label {
            background-color: var(--lavender);
            color: var(--black);
        }
        
        .outputs-layer .layer-label {
            background-color: var(--mint-green);
            color: var(--black);
        }
    </style>
</head>
<body>
    <!-- Slide 1: Title -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content center-content">
            <div class="image-container">
                <img src="lego_banner.png" alt="Multimodal Input Processing Architectures" />
            <!-- <h1>The Future of Multimodal AI Applications</h1>
            <h3>Beyond Text-Based Paradigms: Building Systems that See, Hear, and Respond in Real-Time</h3>
            <p>Stefania Druga</p>
            <p>Research Scientist at Google DeepMind</p>
            <a href="https://shift.infobip.com/us/schedule/" class="schedule-link">View Event Schedule</a> -->
        </div>
    </section>

    <!-- Slide 2: Introduction -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Why Multimodal AI Matters</h2>
            <div class="grid">
                <div class="card pink">
                    <h3>Beyond Text</h3>
                    <p>Traditional AI systems rely primarily on text, limiting their ability to understand and interact with the rich, multimodal world humans naturally navigate.</p>
                </div>
                <div class="card purple">
                    <h3>Real-Time Interaction</h3>
                    <p>The most impactful AI systems don't just analyze - they respond dynamically to changing inputs from multiple streams with minimal latency.</p>
                </div>
                <div class="card green">
                    <h3>Educational Impact</h3>
                    <p>Multimodal AI enables new approaches to learning through context-aware, personalized guidance that adapts to student needs across different representations.</p>
                </div>
            </div>
            <p>Today we'll explore the technical implementation of systems that integrate and synthesize information from webcams, audio feeds, sensors, and more - creating AI that truly understands its environment.</p>
        </div>
    </section>

    <!-- Slide 5: ChemBuddy Case Study -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Case Study 1: ChemBuddy</h2>
            <h3>AI-Powered Chemistry Lab Assistant</h3>
            
            <p>ChemBuddy integrates webcam input, Jacdac sensors, and multimodal conversational AI to provide real-time, context-aware support for middle-school chemistry experiments.</p>
            
            <div class="sensor-diagram">
                <div class="sensor-blocks">
                    <div>
                        <div class="sensor-block sensor-camera">CAM</div>
                        <div class="sensor-text">Visual Analysis</div>
                    </div>
                    <div>
                        <div class="sensor-block sensor-audio">AUDIO</div>
                        <div class="sensor-text">Voice Interaction</div>
                    </div>
                    <div>
                        <div class="sensor-block sensor-data">DATA</div>
                        <div class="sensor-text">Sensor Readings</div>
                    </div>
                </div>
            </div>
            
            <div class="code-block">
                <div class="code-header">ChemBuddy Core Multimodal Integration</div>
                <pre><code class="language-javascript">// WebSocket message handling for real-time multimodal fusion
wss.on('connection', (ws) => {
  let pendingImageRequest = null;
  
  ws.on('message', async (message) => {
    // Handle binary image data
    if (message instanceof Buffer && pendingImageRequest) {
      const base64Image = message.toString('base64');
      const { text, sensorContext } = pendingImageRequest;
      
      // Multimodal fusion: Combine image + text + sensor data
      const result = await callGeminiApi({
        text,
        sensorContext, // Temperature, pH, etc.
        imageBase64: `data:image/jpeg;base64,${base64Image}`
      });
      
      // Send the AI response back to client
      ws.send(JSON.stringify({
        type: 'ai_response', 
        payload: { text: result.message }
      }));
      
      pendingImageRequest = null;
    } 
    // Handle JSON messages (text + sensor data)
    else {
      try {
        const data = JSON.parse(message.toString());
        if (data.type === 'chat_message_request') {
          const { text, sensorContext, hasImage } = data.payload;
          
          hasImage 
            ? pendingImageRequest = { text, sensorContext }
            : processTextWithSensorContext(text, sensorContext);
        }
      } catch (error) {
        ws.send(JSON.stringify({ type: 'error' }));
      }
    }
  });
});</code></pre>
            </div>
            
            <p>ChemBuddy addresses critical gaps in chemistry education by:</p>
            <ul>
                <li>Offering accessible, real-time guidance during physical experiments</li>
                <li>Integrating physical sensor data with AI-driven pedagogical support</li>
                <li>Providing a comprehensive multimodal interaction framework tailored for science education</li>
            </ul>
        </div>
    </section>

    <!-- Slide 4: The Multimodal AI Blueprint -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">The Multimodal AI Blueprint</h2>
            
            <div class="multimodal-arch">
                <div class="arch-layer inputs-layer">
                    <div class="layer-label">Multimodal Inputs</div>
                    <div class="layer-components">
                        <div class="component">Camera</div>
                        <div class="component">Microphone</div>
                        <div class="component">Sensors</div>
                        <div class="component">Text Input</div>
                    </div>
                </div>
                
                <div class="arch-layer processing-layer">
                    <div class="layer-label">Real-Time Processing</div>
                    <div class="layer-components">
                        <div class="component">Multimodal Fusion</div>
                        <div class="component">Context Modeling</div>
                        <div class="component">Knowledge Integration</div>
                    </div>
                </div>
                
                <div class="arch-layer outputs-layer">
                    <div class="layer-label">Output Modalities</div>
                    <div class="layer-components">
                        <div class="component">Visual</div>
                        <div class="component">Audio</div>
                        <div class="component">Haptic</div>
                    </div>
                </div>
            </div>
            
            <h3>Key Technical Challenges</h3>
            <ul>
                <li><strong>Latency Reduction:</strong> Processing multiple data streams while maintaining real-time response</li>
                <li><strong>Data Synchronization:</strong> Ensuring temporal alignment between modalities (32-45ms window)</li>
                <li><strong>Contextual Understanding:</strong> Fusing data from different sources into a coherent understanding</li>
                <li><strong>Resource Efficiency:</strong> Optimizing for deployment on edge devices</li>
            </ul>
        </div>
    </section>

    <!-- Slide 5: MathMind Case Study -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Case Study 2: MathMind</h2>
            <h3>Visual Misconception Detection for Algebra Learning</h3>
            
            <p>MathMind detects algebraic misconceptions from student work using computer vision, provides targeted feedback, and generates personalized practice exercises.</p>
            
            <div class="code-block">
                <div class="code-header">MathMind Image Analysis Pipeline</div>
                <pre><code class="language-typescript">// Misconception detection from student work images
app.post('/api/analyze', async (req, res) => {
  const { imageData } = req.body;
  
  // Configure and prepare Gemini vision model
  const genAI = new GoogleGenerativeAI(apiKey);
  const model = genAI.getGenerativeModel({
    model: "gemini-2.5-pro"
  });
  
  // Process image and create multimodal prompt
  const imagePart = {
    inlineData: {
      data: imageData.replace(/^data:image\/.*;base64,/, ''),
      mimeType: 'image/jpeg'
    }
  };
  
  // Send image with algebra misconception taxonomy prompt
  const result = await model.generateContent([
    misconceptionTaxonomyPrompt, // Structured prompt with 55 misconception types
    imagePart
  ]);
  
  // Extract structured analysis results
  const analysisResults = parseGeminiAnalysisResponse(result.response.text());
  
  res.json(analysisResults);
});</code></pre>
            </div>
            
            <h3>Key Implementation Challenges</h3>
            <div class="grid">
                <div class="card pink">
                    <h3>Vision-Based Detection</h3>
                    <p>Extracting mathematical expressions from handwritten work and classifying specific error patterns</p>
                </div>
                <div class="card purple">
                    <h3>Misconception Taxonomy</h3>
                    <p>Building a comprehensive framework of 55 common algebra misconceptions</p>
                </div>
                <div class="card green">
                    <h3>Adaptive Exercise Generation</h3>
                    <p>Creating personalized problems that target specific detected misconceptions</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Slide 6: Cognimates Case Study -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Case Study 3: Cognimates Copilot</h2>
            <h3>AI-Assisted Block-Based Programming</h3>
            
            <p>Cognimates Scratch Copilot provides real-time support for ideation, code generation, debugging, and asset creation - all while maintaining child agency and creative control.</p>
            
            <div class="code-block">
                <div class="code-header">Cognimates Image Generation with Background Removal</div>
                <pre><code class="language-javascript">// Child-friendly image generation with background removal
const generateImage = useCallback(async (prompt) => {
  setIsLoading(true);
  
  // Add placeholder message and notify user
  const requestMessage = {
    sender: 'user',
    text: `Generating image for: "${prompt}"`
  };
  setMessages(prev => [...prev, requestMessage]);

  try {
    // 1. Generate image using Gemini
    const result = await genAI.getGenerativeModel({ 
      model: IMAGE_MODEL 
    }).generateContent(prompt);
    
    const imagePart = result.response.candidates[0].content.parts[0];
    const rawImageData = imagePart.inlineData.data;
    const mimeType = imagePart.inlineData.mimeType;
    
    // 2. Clean up image by removing background (important for educational context)
    const processedImageBlob = await removeBackground(
      `data:${mimeType};base64,${rawImageData}`
    ); 

    // 3. Convert back to base64 for display
    const processedImageBase64 = await new Promise((resolve) => {
      const reader = new FileReader();
      reader.onloadend = () => resolve(reader.result);
      reader.readAsDataURL(processedImageBlob);
    });
    
    // 4. Create message with processed image
    const aiMessage = {
      sender: 'ai',
      text: `Here's the image for "${prompt}":`,
      imageBase64: processedImageBase64
    };
    
    // 5. Update chat with new image
    setMessages(prev => {
      const updatedMessages = prev.filter(msg => msg !== requestMessage);
      return [...updatedMessages, aiMessage];
    });
    
    return { response: aiMessage };
  } catch (error) {
    // Error handling
  } finally {
    setIsLoading(false);
  }
}, []);</code></pre>
            </div>
            
            <h3>Key Research Findings</h3>
            <ul>
                <li><strong>Agency Negotiation:</strong> Children actively adapted or rejected AI suggestions to maintain creative control</li>
                <li><strong>Design Tension:</strong> Balance between helpful scaffolding and fostering independent problem-solving</li>
                <li><strong>Learning from Limitations:</strong> AI errors created unexpected learning opportunities and critical thinking</li>
                <li><strong>Creative Self-Efficacy:</strong> Carefully designed AI assistance enhanced children's creative confidence</li>
            </ul>
        </div>
    </section>

    <!-- Slide 7: Core Patterns for Multimodal AI - Part 1 -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Core Patterns for Real-Time Multimodal Applications</h2>
            <h3>Input Processing Architectures</h3>
            
            <div class="image-container">
                <img src="/api/placeholder/800/400" alt="Multimodal Input Processing Architectures" />
            </div>
            
            <div class="grid">
                <div class="card pink">
                    <h3>Parallel Processing</h3>
                    <p>Process each modality concurrently through specialized pipelines before fusion.</p>
                    <ul>
                        <li>Optimal for diverse modalities</li>
                        <li>Better utilization of system resources</li>
                    </ul>
                </div>
                <div class="card purple">
                    <h3>Hierarchical Processing</h3>
                    <p>Progressive abstraction of features across modalities from low to high-level.</p>
                    <ul>
                        <li>Mirrors human perception systems</li>
                        <li>Enables cross-modal transfer</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Slide 8: Core Patterns for Multimodal AI - Part 2 -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Core Patterns for Real-Time Multimodal Applications</h2>
            <h3>Model Selection & Deployment</h3>
            
            <div class="grid">
                <div class="card pink">
                    <h3>Cloud Models</h3>
                    <p>Higher capability, larger parameter count models accessed via API</p>
                    <ul>
                        <li>Google's Gemini 2.5 Pro</li>
                        <li>OpenAI's GPT-4V</li>
                        <li>Anthropic's Claude 3.5 Sonnet</li>
                    </ul>
                </div>
                <div class="card purple">
                    <h3>Edge-Optimized Models</h3>
                    <p>Compact models designed for on-device deployment</p>
                    <ul>
                        <li>SmolVLM (256M-2.2B parameters)</li>
                        <li>Moondream (1.6B parameters)</li>
                        <li>Qwen2VL (1.5B parameters)</li>
                    </ul>
                </div>
                <div class="card green">
                    <h3>Specialized Architectures</h3>
                    <p>Models with unique designs for specific requirements</p>
                    <ul>
                        <li>SSMs for ultra-low latency</li>
                        <li>MiniCPM-V for visual reasoning</li>
                        <li>Whisper/Riva for real-time speech</li>
                    </ul>
                </div>
            </div>
            
            <div class="image-container">
                <img src="/api/placeholder/800/300" alt="Model Size vs. Capability Comparison" />
            </div>
        </div>
    </section>
    
    <!-- Slide 9: Core Patterns for Multimodal AI - Part 3 -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Core Patterns for Real-Time Multimodal Applications</h2>
            <h3>Fusion & Context Modeling Approaches</h3>
            
            <div class="image-container">
                <img src="/api/placeholder/800/350" alt="Fusion Techniques Visualization" />
            </div>
            
            <div class="grid">
                <div class="card pink">
                    <h3>Attention-Based Fusion</h3>
                    <p>Using transformer architectures to create dynamic relationships between modalities</p>
                    <ul>
                        <li>Cross-modal attention mechanisms</li>
                        <li>Self-attention within modalities</li>
                        <li>Contextual weighting of features</li>
                    </ul>
                </div>
                <div class="card purple">
                    <h3>Context Management</h3>
                    <p>Maintaining coherent understanding across time and modalities</p>
                    <ul>
                        <li>Knowledge graph integration</li>
                        <li>Retrieval-augmented generation</li>
                        <li>Hierarchical context windows</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Slide 10: Latency & Synchronization Strategies - Part 1 -->
    <section class="slide">
        <div class="gradient-bg"></div>
        <div class="slide-content">
            <h2 class="section-header">Latency & Synchronization Strategies</h2>
            <h3>The Critical Timing Challenge</h3>
            
            <div class="image-container">
                <img src="/api/placeholder/800/350" alt="Multimodal Synchronization Windows Diagram" />
            </div>
            
            <p>Recent research reveals critical timing requirements for coherent multimodal processing:</p>
            <ul>
                <li><strong>General Coherence Window:</strong> 32-45ms maximum delay between modalities</li>
                <li><strong>Tactile/Visual/Audio Integration:</strong> Even tighter 3.8ms window for certain applications</li>
                <li><strong>Performance Degradation:</strong> Systems show 18-24% accuracy drop when windows are missed</li>
            </ul>
            
            <div class="citation">Sources: "Advances in Computer AI-assisted Multimodal Data Fusion Techniques" (2024), "Real-Time Multimodal Signal Processing for HRI" (2025)</div>
        </div>
    </section>
    
    <script>
        // Initialize Prism.js for syntax highlighting
        document.addEventListener('DOMContentLoaded', (event) => {
            Prism.highlightAll();
        });
    </script>
</body>
</html>