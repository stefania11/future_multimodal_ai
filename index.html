<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Keynote: The Future of Multimodal AI Applications</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-typescript.min.js" defer></script>

    <style>
        :root {
            --black: #000000;
            --white: #FFFFFF;
            --soft-pink: #FFD6E0;
            --lavender: #E0D6FF;
            --mint-green: #C1F0DB;
            --dark-gray: #333333;
            --medium-gray: #666666;
            --light-gray-bg: #f7f7f7; /* For code blocks and backgrounds */
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'DM Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }

        html {
             scroll-behavior: smooth; /* Smooth scrolling between slides */
        }

        body {
            background-color: var(--white);
            color: var(--black);
            line-height: 1.6;
        }

        /* Presentation container */
        #presentation-container {
            height: 100vh;
            overflow-y: scroll; /* Enable vertical scrolling */
            scroll-snap-type: y mandatory; /* Snap scrolling to slides */
            position: relative;
        }

        .slide {
            width: 100%;
            min-height: 100vh; /* Ensure slide takes full viewport height */
            padding: 4rem 5%; /* Responsive padding */
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center; /* Center content horizontally */
            scroll-snap-align: start; /* Snap to the start of the slide */
            position: relative;
            overflow: hidden; /* Prevent content overflow issues */
            background-color: var(--white); /* Default background */
        }

        .slide-content {
            max-width: 1200px;
            width: 100%;
            z-index: 1; /* Content above background */
            text-align: left; /* Default text alignment */
        }

        /* Optional subtle gradient background for slides */
        .gradient-bg {
            background: linear-gradient(135deg, rgba(255, 214, 224, 0.1), rgba(224, 214, 255, 0.1), rgba(193, 240, 219, 0.1));
        }

        /* Headings */
        h1, h2, h3, h4, h5, h6 {
            margin-bottom: 1.5rem;
            font-weight: 700;
            line-height: 1.3;
        }

        h1 {
            font-size: 3.5rem !important; /* Ensure fixed size, force override */
            color: var(--dark-gray); /* Set color to dark gray */
            display: inline-block;
        }

        h2 { /* Slide Titles */
            font-size: 3rem !important; /* Fixed size override, removed clamp, force override */
            color: var(--black);
            padding-bottom: 0.5rem;
            position: relative;
            margin-bottom: 2rem; /* More space after title */
        }

        /* Underline effect for h2 */
        h2::after {
             content: '';
             position: absolute;
             bottom: 0;
             left: 0;
             width: 60px;
             height: 4px;
             background: linear-gradient(90deg, var(--soft-pink), var(--lavender), var(--mint-green));
             border-radius: 2px;
         }
         .center-content h2::after {
            left: 50%;
            transform: translateX(-50%);
        }


        h3 { /* Subtitles / Section Heads */
            font-size: clamp(1.5rem, 4vw, 2rem);
            color: var(--dark-gray);
        }

        p, li {
            margin-bottom: 1rem;
            color: var(--dark-gray);
            font-size: clamp(1rem, 2.5vw, 1.2rem); /* Responsive font size */
        }
        ul {
            list-style-position: outside;
            padding-left: 1.5rem;
        }

        /* Grid layout */
        .grid-layout { /* Renamed from 'grid' to avoid conflicts */
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); /* Responsive columns */
            gap: 2rem;
            margin: 2rem 0;
        }

        /* Card styling */
        .card {
            background-color: var(--white);
            border-radius: 1rem;
            padding: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 4px solid;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }
        .card.pink { border-color: var(--soft-pink); }
        .card.purple { border-color: var(--lavender); }
        .card.green { border-color: var(--mint-green); }
        .card.blue {
            background: linear-gradient(to right, #EFF6FF, #DBEAFE); /* blue-50 to blue-100 */
            border-left: 4px solid #3B82F6; /* blue-500 */
        }
        .card h3 { margin-top: 0; font-size: 1.5rem; }

        /* Code block styling */
        .code-block {
            margin: 1.5rem 0;
            border-radius: 0.5rem;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        .code-header {
            background-color: var(--dark-gray);
            color: var(--white);
            padding: 0.5rem 1rem;
            font-family: monospace;
            font-size: 0.9rem;
            border-radius: 0.5rem 0.5rem 0 0; /* Match top radius */
        }
        /* Prism overrides */
        pre[class*="language-"] {
            margin: 0 !important;
            padding: 1.5rem !important; /* More padding */
            border-radius: 0 0 0.5rem 0.5rem !important; /* Match bottom radius */
            background-color: var(--light-gray-bg) !important;
            font-size: 0.9rem !important;
            line-height: 1.5 !important;
            max-height: 40vh; /* Limit height and allow scroll */
            overflow: auto !important;
        }
        code[class*="language-"] {
            font-family: 'Fira Code', monospace !important;
            color: var(--dark-gray) !important; /* Adjust text color for light theme */
        }
        /* Adjust Prism theme colors slightly if needed */
        .token.comment, .token.prolog, .token.doctype, .token.cdata { color: slategray; }
        .token.punctuation { color: #999; }
        /* ... other token adjustments if necessary */

        /* Image container */
        .image-container {
            width: 100%;
            margin: 2rem 0;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            display: flex; /* Center image */
            justify-content: center;
            align-items: center;
            background-color: var(--light-gray-bg); /* BG for placeholders */
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            max-height: 50vh; /* Limit image height */
            display: block;
            object-fit: contain; /* Ensure image fits nicely */
        }

        /* Placeholder Box Styling */
        .placeholder-box {
            border: 2px dashed var(--lavender);
            padding: 2rem;
            text-align: center;
            color: var(--medium-gray);
            border-radius: 0.5rem;
            background-color: rgba(224, 214, 255, 0.1);
            min-height: 200px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            margin: 2rem 0;
        }

        /* Blueprint diagram styling */
        .blueprint-diagram {
             display: flex;
             flex-direction: column;
             align-items: center;
             gap: 1rem;
             margin: 2rem 0;
        }
        .blueprint-row {
            display: flex;
            justify-content: center;
            gap: 1rem;
            flex-wrap: wrap; /* Allow wrapping on smaller screens */
        }
        .blueprint-item {
            background-color: var(--white);
            border-radius: 0.5rem;
            padding: 1rem 1.5rem;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
            text-align: center;
            min-width: 120px;
            border: 1px solid #eee;
        }
        .blueprint-item strong {
            display: block;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }
        .blueprint-arrow {
            font-size: 1.5rem;
            color: var(--medium-gray);
            align-self: center;
            margin: 0 0.5rem;
        }
        .bp-input { border-left: 4px solid var(--soft-pink); }
        .bp-context { border-left: 4px solid var(--soft-pink); }
        .bp-grounding { border-left: 4px solid var(--lavender); }
        .bp-core { border-left: 4px solid var(--lavender); background-color: rgba(224, 214, 255, 0.1); }
        .bp-output { border-left: 4px solid var(--mint-green); }
        .bp-interface { border-left: 4px solid var(--mint-green); }


        /* Specific slide layouts */
        .title-slide .slide-content {
            text-align: center;
        }
        .title-slide h3 { /* Subtitle on title slide */
            font-size: clamp(1.2rem, 3vw, 1.8rem);
            font-weight: 500;
            color: var(--dark-gray);
            margin-bottom: 1.5rem;
        }
        .title-slide .affiliation {
            font-size: clamp(1.1rem, 3vw, 1.5rem);
            color: var(--dark-gray);
            margin-top: 1rem;
            margin-bottom: 1.5rem;
        }
        .title-slide .conference-info {
             font-size: clamp(0.9rem, 2vw, 1.1rem);
             color: var(--medium-gray);
             margin-top: 2rem;
        }
        .title-slide .schedule-link {
             display: inline-block;
             padding: 0.75rem 1.5rem;
             background: linear-gradient(90deg, var(--soft-pink), var(--lavender));
             color: var(--black);
             text-decoration: none;
             border-radius: 99px; /* Pill shape */
             font-weight: 600;
             margin-top: 1rem;
             transition: transform 0.2s ease, box-shadow 0.2s ease;
             box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
         }
        .title-slide .schedule-link:hover {
             transform: translateY(-3px);
             box-shadow: 0 6px 15px rgba(0, 0, 0, 0.15);
         }

        .two-column-layout {
            display: grid;
            grid-template-columns: 1fr 1fr; /* Two equal columns */
            gap: 3rem;
            align-items: center; /* Vertically align items in columns */
            margin-top: 2rem;
        }

        /* References slide styling */
        .references-slide ul {
            list-style-type: none;
            padding-left: 0;
            font-size: 0.8rem;
            line-height: 1.5;
            max-height: 60vh; /* More height for refs */
            overflow-y: auto;
            columns: 2; /* Display references in two columns */
            column-gap: 2rem;
        }
        .references-slide li {
            margin-bottom: 0.5rem;
            word-break: break-word;
        }

        /* Citation styling */
        .citation {
            font-size: 0.8rem;
            color: #666;
            margin-top: 1.5rem;
            text-align: right;
        }

        /* Navigation Buttons */
        #navigation {
            position: fixed;
            right: 2rem;
            top: 50%;
            transform: translateY(-50%);
            z-index: 100;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }
        .nav-button {
            background: linear-gradient(to right, var(--soft-pink), var(--lavender), var(--mint-green));
            color: var(--black);
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 9999px; /* Pill shape */
            cursor: pointer;
            font-weight: 600;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            font-size: 1rem;
        }
        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 15px rgba(0, 0, 0, 0.15);
        }
        .nav-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
        
        /* Make sure slides take up full viewport height for better scrolling */
        .slide {
            min-height: 100vh;
            box-sizing: border-box;
            padding: 40px;
            position: relative;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            justify-content: center;
        }
        
        /* Ensure the container allows scrolling */
        #presentation-container {
            height: 100vh;
            overflow-y: auto;
            scroll-snap-type: y mandatory;
            scrollbar-width: none; /* Hide scrollbar in Firefox */
            -ms-overflow-style: none; /* Hide scrollbar in IE and Edge */
        }
        
        #presentation-container::-webkit-scrollbar {
            display: none; /* Hide scrollbar in Chrome/Safari/Opera */
        }
        
        /* Make each slide snap to scroll position */
        .slide {
            scroll-snap-align: start;
        }

        /* Fullscreen Image Slide Style - Clean reset */
        .fullscreen-image {
            background-size: contain !important;
            background-position: center !important;
            background-repeat: no-repeat !important;
            position: relative;
            height: 100vh;
            width: 100%;
            overflow: hidden;
            padding: 0;
            margin: 0;
        }
        
        .fullscreen-image .slide-content {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        
        .fullscreen-image h2 {
            position: absolute;
            left: 40px;
            top: 40px;
            color: black;
            font-size: 2.5rem;
            background-color: rgba(255, 255, 255, 0.7);
            padding: 10px 20px;
            border-radius: 5px;
            margin: 0;
            z-index: 10;
            max-width: 70%;
        }
    </style>
    
</head>
<body>
    <div id="presentation-container">
        <section class="slide title-slide" id="slide-0">
            <div class="slide-content">
                <h1>The Future of Multimodal AI Applications</h1>
                <div class="image-container">
                    <img src="multimodal_lego2.png" alt="Multimodal Lego Blocks">
                </div>
            </div>
            <div class="slide-credit">Stefania Druga, Chai Seminar | May 12, 2025</div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Beyond Text: AI That Perceives the World</h2>
                <p class="text-xl mb-6">Imagine AI that doesn't just process text, but <strong>perceives</strong> the world alongside us – seeing our experiments, hearing our questions, sensing the environment.</p>
                <div class="grid-layout">
                    <div class="card pink">
                        <h3>The Limitation</h3>
                        <p>Current text-centric AI often misses the richness of real-world context and lacks direct perception.</p>
                    </div>
                    <div class="card purple">
                         <h3>The Need</h3>
                         <p>Why AI that sees, hears, senses? To build more intuitive, grounded, and truly helpful systems.</p>
                    </div>
                </div>
                 <!-- <div class="mt-8 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
                    <p class="font-semibold text-black text-lg">Example:</p>
                    <p>An AI tutor not just reading a math problem, but *seeing* where a student makes a mistake on the paper, in real-time.</p>
                </div> -->
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Vision: Real-Time Multimodal AI</h2>
                <!-- Improved layout with grid - text takes 3/5 of space, image takes 2/5 -->
                <div class="grid grid-cols-5 gap-8 items-center">
                    <div class="col-span-3">
                        <p class="text-xl mb-6">AI systems that seamlessly integrate and synthesize information from diverse, real-time data streams:</p>
                        <ul class="list-disc list-inside space-y-2 text-lg">
                            <li>Live Webcams & Video Feeds</li>
                            <li>Microphone Audio</li>
                            <li>Connected Sensors (Temperature, Location, etc.)</li>
                        </ul>
                        <p class="mt-6 text-lg">Understanding context, anticipating needs, and responding dynamically through multi-sensory feedback loops.</p>
                    </div>
                    <div class="col-span-2 flex justify-center">
                        <img src="sensors.png" alt="Sensors Context Analysis Diagram" class="max-h-[50vh] object-contain">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Why Multimodal AI Matters</h2>
                <div class="grid-layout">
                    <div class="card pink">
                        <h3>Beyond Text</h3>
                        <p>Traditional AI systems rely primarily on text, limiting their ability to understand and interact with the rich, multimodal world humans naturally navigate.</p>
                    </div>
                    <div class="card purple">
                        <h3>Real-Time Interaction</h3>
                        <p>The most impactful AI systems don't just analyze - they respond dynamically to changing inputs from multiple streams with minimal latency.</p>
                    </div>
                    <div class="card green">
                        <h3>Grounding</h3>
                        <p>By connecting language to sensory inputs (vision, audio, sensors), multimodal AI anchors abstract concepts in real-world perception, leading to deeper understanding and reduced ambiguity.</p>
                    </div>
                </div>
            </div>
        </section>

         <section class="slide fullscreen-image" style="background-image: url('blueprint.png');">
            <div class="slide-content">
                <h2>The Multimodal AI Blueprint</h2>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2 class="mb-6">Case Studies</h2>
                <p class="text-center text-xl mb-12">Let's see how this blueprint applies in practice with examples from our research...</p>
                 <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6">
                    <div class="card blue">
                        <h3 class="text-2xl mb-2">Gemini Smart Home</h3>
                        <p>Conversational control of smart devices directly within the Gemini AI.</p>
                    </div>
                    <div class="card pink">
                        <h3 class="text-2xl mb-2">ChemBuddy</h3>
                        <p>Making abstract chemistry tangible through real-time sensing and interaction.</p>
                    </div>
                     <div class="card purple">
                        <h3 class="text-2xl mb-2">MathMind</h3>
                        <p>Visually identifying and addressing mathematical misconceptions on the fly.</p>
                    </div>
                     <div class="card green">
                        <h3 class="text-2xl mb-2">Cognimates Copilot</h3>
                        <p>Supporting creative coding beyond text with multimodal assistance.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Case Study: Gemini & Google Home Integration</h2>
                <!-- Modified layout: text 60%, image 40% -->
                <div class="content-with-image grid grid-cols-5 gap-8 items-center">
                    <div class="text-content col-span-3">
                        <p class="text-xl mb-4">Bringing natural language smart home control directly into the Gemini AI chat experience.</p>
                        <h3 class="mb-2 text-lg font-semibold">Core Idea:</h3>
                        <ul class="list-disc list-outside space-y-1">
                            <li>Control lights, climate, media, etc. via Gemini prompts.</li>
                            <li>Example: "Set the dining room for a romantic date night."</li>
                            <li>Reduces friction by keeping control within the AI chat context.</li>
                        </ul>
                        <p class="mt-4 text-sm text-gray-600">Project Goal: Make smart home interaction more intuitive and conversational.</p>
                    </div>
                    <!-- Added max-h-full to image wrapper and adjusted col-span -->
                    <div class="image-wrapper col-span-2 flex justify-center items-center max-h-full">
                        <img src="google_home_gemini_extension_2.webp" alt="Gemini Google Home Extension UI" class="max-h-[50vh] object-contain">
                    </div>
                </div>
            </div>
        </section>

        <!-- NEW SLIDE: Gemini Home Extension Details & Signup -->
        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Gemini Home Extension: Details & Early Access</h2>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <h3 class="mb-2">Capabilities & Limitations:</h3>
                        <ul class="list-disc list-outside space-y-1 mb-4">
                            <li><strong>Controls:</strong> Lighting, climate, window coverings, TVs, speakers, etc.</li>
                            <li><strong>Requires Home App for:</strong> Security devices (cameras, locks), Routines.</li>
                            <li><strong>Activation:</strong> May need '@Google Home' in prompts initially.</li>
                            <li><strong>Context:</strong> Part of broader industry trend (cf. Alexa, Siri AI upgrades).</li>
                        </ul>
                    </div>
                    <div class="p-6 rounded-lg bg-white shadow-md border border-gray-200">
                        <h3 class="mb-3 text-lg font-semibold">Sign Up for Public Preview:</h3>
                        <p class="mb-4">Get early access to this feature (Android, English only initially) via the Google Home Public Preview program.</p>
                        <a href="https://support.google.com/googlenest/answer/12494697" target="_blank" class="inline-block bg-blue-600 text-white px-4 py-2 rounded hover:bg-blue-700 transition-colors">
                            Join Google Home Public Preview
                        </a>
                         <p class="text-xs mt-3 text-gray-500">Requires signing into Gemini with the same account as Google Home.</p>
                    </div>
                 </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Case Study 1: ChemBuddy</h2>
                <h3 class="text-lg text-gray-600 mb-6">AI-Powered Chemistry Lab Assistant</h3> 
                <div class="content-with-image grid grid-cols-5 gap-8 items-center">
                    <div class="text-content col-span-2">
                        <p class="text-xl mb-4">Making abstract chemistry tangible through real-time sensing and interaction.</p>
                        <h3 class="mb-2 text-lg font-semibold">Core Features:</h3>
                        <ul class="list-disc list-outside ml-5 mb-4 text-lg"> 
                            <li>Real-world pH sensing via Jacdac</li>
                            <li>AI analyzes sensor data & user actions</li>
                            <li>Adaptive guidance based on experiment state</li>
                        </ul>
                        <p class="mt-4 text-md text-gray-600">Goal: Bridge concrete actions with abstract concepts.</p> 
                    </div>
                    <div class="image-wrapper col-span-3 flex justify-center items-center max-h-full">
                        <img src="overview_chembuddy.png" alt="ChemBuddy Overview" class="max-h-[60vh] object-contain rounded-lg shadow-md"> 
                    </div>
                </div>
            </div>
        </section>

        <!-- Insert ChemBuddy slides BEFORE Architecture -->
        <!-- NEW Slide: ChemBuddy Experiments -->
        <section class="slide fullscreen-image" style="background-image: url('chembuddy_experiments 2.png'); background-size: cover; background-position: center;">
            <div class="slide-content">
            </div>
        </section>



        <!-- NEW Slide: ChemBuddy Grounding -->
        <section class="slide fullscreen-image" style="background-image: url('chembuddy_grounding_example 2.png'); background-size: cover; background-position: center;">
            <div class="slide-content">
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>ChemBuddy: Architecture</h2>
                <div class="image-container" style="text-align: center;">
                    <img src="chembuddy_architecture.png" alt="ChemBuddy Architecture Diagram" style="max-height: 450px;">
                </div>
            </div>
        </section>

        <section class="slide" data-slide-index="11">
            <div class="slide-content">
                <h2>ChemBuddy: Implementation & Insight</h2>
                <strong class="text-xl block mb-2 text-black">Implementation Highlight: Real-time Loop (Core Logic)</strong>
                <p class="mb-2">Using WebSockets, the client sends sensor/image data; the server fuses it and calls the AI for immediate feedback.</p>
                <div class="code-block">
                    <div class="code-header">ChemBuddy: WebSocket Multimodal Fusion (Revised)</div>
                    <pre><code class="language-javascript">// Client: Send multimodal update via WebSocket
        async function sendUpdate(text, imageBlob, sensorData) {
          let imageBase64 = null;
          if (imageBlob) {
            imageBase64 = await blobToBase64(imageBlob); // Convert Blob to Base64
          }
          const messagePayload = {
            text: text,
            sensorData: sensorData,
            image: imageBase64 // Embed Base64 image data (or null)
          };
          // Send a single JSON message containing all data
          socket.send(JSON.stringify({ type: 'chem_update_request', payload: messagePayload }));
        }
        
        // Server: Handle WebSocket message
        socket.on('message', async (message) => {
          try {
            // Parse the incoming JSON message
            const received = JSON.parse(message.toString());
        
            if (received.type === 'chem_update_request' && received.payload) {
              const { text, sensorData, image } = received.payload; // Extract data
              let imageBuffer = null;
              if (image) {
                imageBuffer = Buffer.from(image, 'base64'); // Decode Base64 image if present
              }
              // --> Call Multimodal AI (Gemini) with combined data
              // Note: Pass imageBuffer (binary) or image (base64) based on API needs
              const aiResponse = await callGeminiApi({ text, sensorData, imageBuffer });
              // Send AI response back to client
              socket.send(JSON.stringify({ type: 'ai_response', payload: { text: aiResponse } }));
            }
          } catch (e) {
            // Handle potential errors (e.g., JSON parsing)
            console.error("Failed to process message:", e);
          }
        });</code></pre>
                </div>
                <div class="mt-6 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
                    <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
                    <p class="text-lg">Integrating real-time sensor data significantly improved the AI's ability to provide relevant, safety-conscious, and conceptually accurate guidance during experiments. It grounds the conversation in the physical reality of the lab.</p>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Case Study 2: MathMind</h2>
                <h3>Visually Identifying Math Misconceptions</h3>
                 <div class="grid grid-cols-5 gap-8 items-center">
                     <div class="col-span-3">
                         <p class="text-xl mb-4">Visually identifying and addressing mathematical misconceptions on the fly.</p>
                         <h3 class="mb-2">Core Features:</h3>
                         <ul class="list-disc list-outside ml-5 space-y-2">
                             <li>Real-time vision analysis of handwritten work</li>
                             <li>Classification of errors against misconception taxonomy</li>
                             <li>Targeted, multimodal feedback (visual hints, explanations)</li>
                         </ul>
                         <p class="mt-4 text-sm text-gray-600">Goal: Provide timely, personalized scaffolding for math learning.</p>
                     </div>
                     <div class="col-span-2 flex justify-center">
                         <img src="mathmind_with_misconception.png" alt="MathMind Misconception Example" class="rounded-lg shadow-lg object-contain max-h-[60vh]">
                     </div>
                 </div>
            </div>
        </section>

        <!-- Insert MathMind slide BEFORE Architecture -->
        <!-- NEW Slide: Math Misconceptions -->
        <section class="slide fullscreen-image" style="background-image: url('math_misconceptions.png');">
            <div class="slide-content">
            </div>
        </section>

        
        <section class="slide">
             <div class="slide-content">
                 <h2>MathMind: Architecture</h2>
                 <div class="image-container" style="text-align: center;">
                    <img src="mathmind_architecture.png" alt="MathMind Architecture Diagram" style="max-height: 450px;">
                </div>
            </div>
        </section>
        <section class="slide">
            <div class="slide-content">
                <h2>MathMind: Vision API Call for Misconception Detection</h2>
        <div class="code-block">
            <div class="code-header">MathMind: Vision API Call for Misconception Detection</div>
            <pre><code class="language-javascript">// Client: Call backend API to analyze image
        async function analyzeMathWork(imageBase64, promptText, taxonomy) {
          try {
            const response = await fetch('/api/analyze', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ imageBase64, prompt: promptText, taxonomy })
            });
            if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);
            const analysis = await response.json();
            displayFeedback(analysis); // Show results in UI
          } catch (error) {
            console.error("Error analyzing math work:", error);
            // Display error to user
          }
        }
        
        // Server: Process request and call AI (e.g., Express.js)
        app.post('/api/analyze', async (req, res) => {
          try {
            const { imageBase64, prompt, taxonomy } = req.body;
            if (!imageBase64 || !prompt) {
              return res.status(400).json({ error: 'Missing imageBase64 or prompt.' });
            }
        
            const model = getGeminiVisionModel(); 
            // Construct the full prompt including taxonomy if needed by the model API
            const fullPrompt = `${prompt} (Consider taxonomy: ${JSON.stringify(taxonomy)})`;
        
            const aiResponse = await model.generateContent([fullPrompt, { inlineData: { mimeType: 'image/jpeg', data: imageBase64 } }]);
            // Assuming aiResponse structure allows direct sending or needs transformation
            res.json({ analysis: aiResponse.response.candidates[0].content.parts[0].text });
          } catch (error) {
            console.error("Error processing AI analysis:", error);
            res.status(500).json({ error: 'Failed to analyze image.' });
          }
        });</code></pre>
        </div>
        <div class="mt-6 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
            <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
            <p class="text-lg">Direct visual analysis of student work allows for highly specific and timely misconception detection, enabling the generation of truly personalized feedback and practice.</p>
        </div>
        </section>
       
        <section class="slide">
            <div class="slide-content">
                <h2>MathMind: Evaluation Results</h2>    
                <img src="eval_mathmind.png" alt="MathMind Evaluation Example showing student work analysis">
 
        </section>

       <section class="slide">
            <div class="slide-content">
                <h2>Case Study 3: Cognimates Copilot</h2>
                <img src="copilot_overview 2.png" alt="Cognimates Copilot Overview">
            </div>
        </section>

       
        <section class="slide fullscreen-image" style="background-image: url('codelab-ui-in-browser-v3.png');">
            <div class="slide-content">
            
            </div>
        </section>
        <section class="slide fullscreen-image" style="background-image: url('cognimates_image_training_old 2.png');">
            <div class="slide-content">
                
            </div>
        </section>
        <section class="slide fullscreen-image" style="background-image: url('cognimates_training_new.png');">
            <div class="slide-content">
            </div>
        </section>
        <section class="slide">
            <div class="slide-content">
                <h2>Cognimates Copilot: Implementation Details</h2>
                 <strong class="text-xl block mb-2 text-black">Implementation Highlight: AI Call & Response Parsing</strong>
                 <p class="mb-2">User input triggers an AI call; the response (text, code, or image data) is parsed and integrated back into the coding environment (like Scratch).</p>
                <div class="code-block">
                    <div class="code-header">Cognimates: Multimodal Response Handling (Conceptual)</div>
                    <pre><code class="language-javascript">// Core logic when user asks for help or an asset
async function handleUserInput(prompt, context) {
  // --> 1. Call AI Model (e.g., Gemini) with prompt and context
  const aiResponse = await callAIModelAPI(prompt, context);

  // 2. Parse response to determine type (text, code, image)
  const parsed = parseAIResponse(aiResponse);

  // 3. Integrate back into UI
  if (parsed.type === 'image_asset') {
    // --> Optional: Call background removal API/library
    const finalImage = await removeBackground(parsed.imageBase64);
    displayImageAsset(finalImage); // Add to Scratch assets
  } else if (parsed.type === 'code_suggestion') {
    displayCodeSuggestion(parsed.codeBlocks); // Show blocks in UI
  } else {
    displayExplanation(parsed.text); // Show text in chat
  }
}</code></pre>
                </div>
                 <div class="mt-6 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
                    <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
                    <p class="text-lg">Multimodal copilots can significantly lower barriers to creative expression in coding by offering contextual help and asset generation directly within the workflow, moving beyond simple text-based assistance.</p>
                    <p class="mt-2 text-md text-gray-600"><strong>Ethical Touchpoint:</strong> Important to consider user agency and avoid over-reliance, ensuring the AI assists rather than dictates the creative process.</p>
                 </div>
            </div>
        </section>
        <section class="slide">
            <div class="slide-content">
                <h2>Copilot Evaluation</h2>
                <img src="copilot_evaluation_benchmark.png" alt="Copilot Evaluation Benchmark" class="rounded-lg shadow-lg object-contain max-h-[70vh]">
                <p class="mt-2"><strong>Goal:</strong> Evaluate copilot performance in real-world scenarios.</p>
            </div>
        </section>
       
        <section class="slide fullscreen-image" style="background-image: url('copilot_eval_mexico.png');">
            <div class="slide-content">
            </div>
        </section>
        <section class="slide fullscreen-image" style="background-image: url('summary_user_studies_copilot.png');">
            <div class="slide-content">
            </div>
        </section>

        


        <!-- Insert Cognimates slides AFTER Code Syntax -->
        <!-- NEW Slides: Cognimates Features/Sims -->
        <section class="slide fullscreen-image" style="background-image: url('copilot_new_features.png');">
            <div class="slide-content">
            </div>
        </section>
        <section class="slide fullscreen-image" style="background-image: url('copilot_simulations.png');">
            <div class="slide-content">
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2 class="text-center">The Future Trajectory: Beyond Today's Examples</h2>
                <p class="text-center text-xl mb-10">The principles behind these examples point towards a broader future for real-time multimodal AI:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 lg:grid-cols-3">
                    <div class="card pink">
                        <h3>Personalized Assistance</h3>
                        <p>AI understanding context (location, activity, sensors) for proactive help.</p>
                    </div>
                     <div class="card purple">
                        <h3>Accessibility Tools</h3>
                        <p>Real-time translation between visual, audio, and haptic information.</p>
                    </div>
                     <div class="card green">
                        <h3>Robotics</h3>
                        <p>Machines perceiving, understanding, and interacting naturally and safely.</p>
                    </div>
                     <div class="card pink">
                         <h3>Creative Tools</h3>
                         <p>AI partners collaborating via sketches, gestures, voice, code.</p>
                    </div>
                    <div class="card purple">
                        <h3>Enhanced Learning</h3>
                        <p>Truly adaptive education responding to diverse styles and real-world needs.</p>
                    </div>
                    <div class="card green">
                        <h3>Human-AI Collaboration</h3>
                        <p>Richer, intuitive partnerships via shared perception.</p>
                    </div>
                </div>
            </div>
        </section>

      
        <section class="slide">
            <div class="slide-content">
                <h2>Evaluating Multimodal Apps: Benchmarks & Strategies</h2>
                <p>Assessing the capabilities and reliability of multimodal applications requires diverse evaluation methods:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6 items-start">
                    <div class="card pink">
                        <h3>Key Benchmarks</h3>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>MMMU, MathVista, MMStar (General)</li>
                            <li>DocVQA, TextVQA (Documents)</li>
                            <li>Video-MME, CinePile (Video)</li>
                            <li>Domain-Specific (e.g., Healthcare, Robotics)</li>
                        </ul>
                        <p class="mt-2">Evaluating capabilities across diverse tasks and modalities.</p>
                    </div>
                    <div class="card purple">
                        <h3>Testing Strategies</h3>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Component & End-to-End Testing</li>
                            <li>Real-world Scenario Simulation</li>
                            <li>Robustness Testing (Noise, Adversarial)</li>
                            <li>Monitoring & Feedback Loops in Deployment</li>
                            <li>Measuring Latency, Throughput, Resource Use</li>
                        </ul>
                         <p class="mt-2">Ensuring reliability beyond standard metrics.</p>
                    </div>
                </div>
                <p class="text-center">Challenges include metrics reflecting real user experience and testing complex interactions.</p>
            </div>
        </section>

        <!-- NEW SLIDE: Creative Coding Evaluation -->
        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Evaluating Creative Coding Copilots</h2>
                <p class="text-xl mb-6">Assessing AI assistance in open-ended creative tasks requires different approaches than traditional benchmarks.</p>
                <div class="two-column-layout items-center gap-8">
                     <div>
                        <strong class="text-xl block mb-2 text-black">Challenges & Approaches:</strong>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Defining "success" in creative tasks is subjective.</li>
                            <li>Need to evaluate the *process* as much as the *product*.</li>
                            <li>Measuring impact on user learning, exploration, and overcoming blocks.</li>
                            <li>Developing benchmarks that simulate real coding scenarios (e.g., completing a partial project, debugging, generating specific assets).</li>
                        </ul>
                        <strong class="text-xl block mt-4 mb-2 text-black">Example: Cognimates Copilot Evaluation</strong>
                        <p>Using project-based scenarios to evaluate the copilot's ability to provide relevant code suggestions, explain concepts, and generate useful visual assets within the Scratch environment.</p>
                    </div>
                    <div class="flex justify-center items-center">
                        <img src="copilot_evaluation_benchmark.png" alt="Cognimates Copilot Evaluation Benchmark Example" class="rounded-lg shadow-lg object-contain max-h-[55vh]">
                    </div>
                </div>
            </div>
        </section>
        <section class="slide">
            <div class="slide-content">
                <h2>AI Assistants Evaluation: A User-Centered Framework</h2>
                <p>https://github.com/kaushal0494/UnifyingAITutorEvaluation</p>
                <p class="text-lg mb-4">The <a href="https://github.com/kaushal0494/UnifyingAITutorEvaluation" target="_blank" class="text-purple-600 hover:underline">Unifying AI Tutor Evaluation</a> framework proposes a taxonomy to assess the pedagogical abilities of LLM-based tutors across key dimensions.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
                    <!-- Left Column: Image and Description -->
                    <div class="space-y-4">
                         <p>This structured approach uses a detailed JSON format to capture assistant responses and annotate them across dimensions like mistake identification, guidance quality, coherence, and tone.</p>
                         <img src="evaluation.png" alt="Diagram illustrating the AI Tutor Evaluation Framework components" class="rounded-lg shadow-lg object-contain max-h-[45vh] mt-4">
                    </div>
                    <!-- Right Column: Simplified JSON Structure -->
                    <div class="code-block max-h-[65vh] overflow-y-auto">
                        <div class="code-header">Evaluation Structure (Simplified JSON)</div>
                        <pre><code class="language-json">{
  "conversation_id": "...",
  "conversation_history": "...",
  "Ground_Truth_Solution": "...",
  "anno_llm_responses": {
    "Model_Name": {
      "response": "Tutor response...",
      "annotation": {
        "Mistake_Identification": "Yes/No/...",
        "Mistake_Location": "Yes/No/...",
        "Revealing_of_the_Answer": "Yes/No/...",
        "Providing_Guidance": "Yes/No/...",
        "Actionability": "Yes/No/...",
        "Coherence": "Yes/No/...",
        "Tutor_Tone": "Encouraging/Neutral/...",
        "Humanlikeness": "Yes/No/..."
      }
    }
  }
}</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
             <div class="slide-content">
                <h2>Advanced Data Fusion Techniques</h2>
                <p>Integrating diverse data streams (vision, audio, sensor, text) effectively is key. Moving beyond simple concatenation to leverage techniques like:</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center"> 
                    <!-- Text Column -->
                    <div class="space-y-3">
                        <ul class="list-disc list-outside ml-5 text-lg">
                            <li><strong>Deep Learning Fusion:</strong> End-to-end models with modality-specific branches.</li>
                            <li><strong>Attention Mechanisms:</strong> Cross-modal attention to weigh feature importance dynamically.</li>
                            <li><strong>Transformer Variants:</strong> Models like ViLBERT, CoCa for powerful cross-modal interaction.</li>
                            <li><strong>Graph Neural Networks (GNNs):</strong> Modeling relationships in structured data/robotics.</li>
                            <li><strong>Recurrent Architectures:</strong> LSTMs/GRFs adapted for multimodal time-series.</li>
                        </ul>
                        <p class="mt-4 font-medium text-lg"><strong>Goal:</strong> Create unified representations capturing richer context than single modalities alone.</p>
                    </div>
                    <!-- Image Column -->
                    <div class="flex justify-center items-center"> 
                        <img src="fusion.png" alt="Data Fusion Approaches Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]"> 
                    </div>
                </div>
            </div>
        </section>

        <!-- NEW SLIDE: Unifying AI Tutor Evaluation Framework -->
        

        <!-- New Slide 2: Latency/Sync -->
        <section class="slide">
            <div class="slide-content">
                <h2>Tackling Real-Time Challenges: Latency & Synchronization</h2>
                <p>Real-time multimodal systems require overcoming critical engineering hurdles:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6 items-start">
                    <div class="card blue">
                        <h3>Low Latency</h3>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Efficient Models (SmolVLM, SSMs)</li>
                            <li>Optimized Inference (TensorRT-LLM, MLX)</li>
                            <li>Quantization (4-bit, FP8)</li>
                            <li>Edge Computing & Asynchronous Processing</li>
                            <li>API Optimization (Batching, Caching)</li>
                        </ul>
                        <p class="mt-2"><strong>Goal:</strong> Minimize delay for interactive experiences (e.g., &lt;100ms).</p>
                    </div>
                    <div class="card green">
                        <h3>Data Synchronization</h3>
                        <p>Crucial for coherent understanding (e.g., 32-45ms window)</p>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Algorithms: Time/Feature/Model-based Alignment</li>
                            <li>Adaptive Temporal Mapping (Handles Jitter)</li>
                            <li>Cross-Modal Correlation Detection</li>
                            <li>Unified Data Platforms (Minimize Integration Sync Issues)</li>
                        </ul>
                         <p class="mt-2"><strong>Goal:</strong> Ensure temporal alignment across streams despite network variability.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Tackling Real-Time Challenges: Latency & Synchronization</h2>
                <p>Real-time multimodal systems require overcoming critical engineering hurdles:</p>
                <div class="flex justify-between items-center gap-8 mb-4">
                    <div class="w-1/2 flex justify-center">
                        <img src="latency_sync.png" alt="Graph showing latency spikes and synchronization" class="rounded-lg shadow-lg object-contain max-h-[45vh]">
                    </div>
                    <div class="w-1/2 flex justify-center">
                        <img src="cloud_vs_edge.png" alt="Cloud vs Edge" class="rounded-lg shadow-lg object-contain max-h-[45vh]">
                    </div>
                </div>
                <p class="mt-2"><strong>Goal:</strong> Ensure temporal alignment across streams despite network variability.</p>
            </div>
        </section>


        <!-- New Slide 3: Parallel Processing -->
        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Architectures for Parallel Processing</h2>
                <p>Handling multiple data streams efficiently often requires parallel processing strategies:</p>
                 <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                    <div>
                        <ul class="list-disc pl-5 space-y-2 text-lg">
                            <li><strong>Modality-Specific Pipelines:</strong> Dedicated processing paths for vision, audio, sensors before fusion.</li>
                            <li><strong>Asynchronous Task Handling:</strong> Utilizing background tasks/queues for non-critical processing (e.g., logging, detailed analysis).</li>
                            <li><strong>Hardware Acceleration:</strong> Leveraging GPUs, TPUs, or specialized AI chips for computationally intensive tasks.</li>
                            <li><strong>Distributed Systems / Edge Computing:</strong> Processing data closer to the source to reduce central load and latency.</li>
                            <li><strong>Optimized Scheduling:</strong> Efficiently managing compute resources across parallel tasks.</li>
                        </ul>
                        <p class="mt-4"><strong>Goal:</strong> Maximize throughput and responsiveness by handling concurrent data streams effectively.</p>
                    </div>
                    <div class="image-container">
                        <img src="parallel_processing.png" alt="Parallel Processing Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                    </div>
                </div>
            </div>
        </section>



        <!-- New Slide 5: User Testing -->
        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>User Research & Testing in Multimodal AI</h2>
                 <p class="text-xl mb-6">Understanding user interaction and experience is critical, especially in Human-Robot Interaction (HRI) and copilots:</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-start"> 
                    <!-- Text Column -->
                    <div class="space-y-3">
                        <ul class="list-disc list-outside ml-5 text-lg">
                            <li><strong>Observational Studies:</strong> Analyzing how users interact naturally with the system.</li>
                            <li><strong>Task-Based Evaluations:</strong> Measuring success rates, efficiency, and errors on specific tasks.</li>
                            <li><strong>Qualitative Feedback:</strong> Interviews, surveys to capture user perception, satisfaction, and pain points.</li>
                            <li><strong>Analyzing Non-Verbal Cues:</strong> Using AI to understand user state (engagement, confusion) during interaction.</li>
                            <li><strong>Iterative Design:</strong> Incorporating feedback into development cycles.</li>
                        </ul>
                        <p class="mt-4 font-medium text-lg"><strong>Goal:</strong> Build systems that are intuitive, effective, and meet user needs in real-world contexts.</p>
                    </div>
                    <!-- Image Column -->
                    <div class="flex justify-center items-center"> 
                        <img src="user_research.png" alt="User Research and Testing Cycle Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]"> 
                    </div>
                </div>
            </div>
        </section>

       
        <!-- New Slide: Material Science -->
        <section class="slide gradient-bg">
            <div class="slide-content text-center">
                <h2>Future Application: Accelerating Material Science Discovery</h2>
                <div class="image-container mt-8">
                    <img src="new_material_science.png" alt="AI analyzing material structures" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">Multimodal AI can analyze experimental data, simulations, and literature to predict properties of novel materials.</p>
            </div>
        </section>

        <!-- New Slide: CAD Design -->
        <section class="slide">
            <div class="slide-content text-center">
                <h2>Future Application: AI-Assisted CAD & Engineering Design</h2>
                <div class="image-container mt-8">
                    <img src="cad_design_support.png" alt="AI assisting with CAD software" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">AI agents can understand design sketches, suggest optimizations, and automate routine CAD tasks based on multimodal input.</p>
            </div>
        </section>

        <!-- New Slide: Cultural Preservation -->
        <section class="slide gradient-bg">
            <div class="slide-content text-center">
                <h2>Future Application: Enhancing Cultural Heritage Preservation</h2>
                <div class="image-container mt-8">
                    <img src="cultural_preservation.png" alt="AI helping analyze cultural artifacts" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">Analyzing artifacts, translating ancient texts, and creating immersive virtual reconstructions using multimodal data.</p>
            </div>
        </section>

        <!-- New Slide: Language Preservation -->
        <section class="slide">
            <div class="slide-content text-center">
                <h2>Future Application: Documenting & Revitalizing Languages</h2>
                <div class="image-container mt-8">
                    <img src="language_preservation.png" alt="AI assisting with language documentation" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">Using audio, video, and text to document endangered languages, create learning tools, and facilitate translation.</p>
            </div>
        </section>

        <!-- New Slide 7: Synthetic Data/Custom -->
        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Synthetic Data for Evaluation & Fine-tuning</h2>
                <p>Adapting models for specific tasks or domains often involves specialized training techniques:</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                    <div>
                        <ul class="list-disc pl-5 space-y-2 text-lg">
                            <li><strong>Fine-tuning:</strong> Adapting pre-trained models (e.g., VLMs) on domain-specific datasets.</li>
                            <li><strong>Synthetic Data Generation:</strong> Creating artificial data (images, sensor readings, text) to augment limited real-world data, especially for rare events or specific scenarios.</li>
                            <li><strong>Few-Shot / Zero-Shot Learning:</strong> Enabling models to perform tasks with minimal or no specific training examples.</li>
                            <li><strong>Custom Sensor Models:</strong> Training smaller models on specific sensor inputs (e.g., IMU for activity recognition) for efficiency and specialized tasks.</li>
                            <li><strong>Parameter-Efficient Fine-Tuning (PEFT):</strong> Methods like LoRA to adapt large models with fewer trainable parameters.</li>
                        </ul>
                        <p class="mt-4"><strong>Goal:</strong> Improve performance on target tasks, handle data scarcity, and enable deployment on resource-constrained devices.</p>
                    </div>
                    <div class="image-container">
                        <img src="cognimates_training_new.png" alt="Cognimates Model Training Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                    </div>
                </div>
            </div>
        </section>

       

        <section class="slide references-slide">
            <div class="slide-content">
               <h2>References</h2>
               <ul>
                   <li>ArXiv. (2024). Multimodality of AI for Education: Towards Artificial General Intelligence.</li>
                   <li>ArXiv. (2024). Multimodal Alignment and Fusion: A Survey.</li>
                    <li>Cartesia Raises $27 Million to Build the Next Generation of Real-Time AI Models - PRWeb (2025)</li>
                   <li>GM Insights. (2025). Multimodal AI Market Size & Share, Statistics Report 2025-2034.</li>
                    <li>LiveKit Blog. (2024). An open source stack for real-time multimodal AI.</li>
                   <li>MIT Technology Review. (2024). Multimodal: AI's new frontier.</li>
                    <li>Mobius Labs - Efficient Multimodal AI for Enterprise Applications (EliteAI Tools)</li>
                   <li>Multimodal Fusion Artificial Intelligence Model to Predict Risk for MACE and Myocarditis... (PMC, 2024)</li>
                   <li>Nature. (2025). On opportunities and challenges of large multimodal foundation models in education.</li>
                    <li>NVIDIA Riva - Speech and Translation AI (NVIDIA)</li>
                    <li>ResearchGate. (2025). SmolVLM: Redefining small and efficient multimodal models.</li>
                   <li>Science Direct. (2025). Taking the next step with generative artificial intelligence: The transformative role of multimodal large language models in science education.</li>
                    <li>U.S. Department of Education. (2024). AI Report.</li>
                    <li>World Economic Forum. (2024). The future of learning: AI is revolutionizing education 4.0.</li>
                    <li>Zilliz. (2024). Top 10 Multimodal AI Models of 2024.</li>
                    <li>Druga, S. et al. (Relevant publications for ChemBuddy, MathMind, Cognimates - *Add Specific Citations*)</li>
                    <li>Advances in Computer AI-assisted Multimodal Data Fusion Techniques (ResearchGate, 2024)</li>
                    <li>Real-Time Multimodal Signal Processing for HRI in RoboCup... (ResearchGate, 2025)</li>
                    <li>Designing the User Interface for Multimodal Speech and Pen-Based Gesture Applications... (ResearchGate)</li>
                    <li>SmolVLM: Redefining small and efficient multimodal models (arXiv, 2025)</li>
                    <li>USER-VLM 360°: Personalized Vision Language Models... (arXiv, 2025)</li>
                    <li>Retrieval Augmented Generation and Understanding in Vision... (arXiv, 2025)</li>
                    <li>GraphRAG with MongoDB Atlas... (MongoDB Blog, 2024)</li>
                    <li>An open source stack for real-time multimodal AI (LiveKit Blog, 2024)</li>
                    <li>Build Real-Time Multimodal XR Apps with NVIDIA AI Blueprint... (NVIDIA Blog, 2024)</li>
                    <li>AIRLab-POLIMI/ROAMFREE (GitHub)</li>
                    <li>MPE™ IMU & Sensor Fusion Software Solutions (221e)</li>
                    <li>Development of an artificial intelligence-based multimodal diagnostic system for early detection of biliary atresia (PMC, 2024)</li>
                    <li>MuDoC: An Interactive Multimodal Document-grounded Conversational AI System (arXiv, 2025)</li>
                    <li>Kaushal, V., et al. (2024). Unifying AI Tutor Evaluation... GitHub.</li>

               </ul>
            </div>
       </section>

       <section class="slide">
            <div class="slide-content">
                <img src="thank_you_slide.png" alt="Thank You Slide">
            </div>
       </section>

    </div> <!-- End of presentation-container -->

    <!-- Navigation Buttons -->
    <div id="navigation">
        <button id="prevBtn" class="nav-button" title="Previous Slide (Up Arrow)">▲</button>
        <button id="nextBtn" class="nav-button" title="Next Slide (Down Arrow)">▼</button>
    </div>
    <div id="slideCounter"></div>

    <script>
        const presentationContainer = document.getElementById('presentation-container');
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');
        const slideCounter = document.getElementById('slideCounter');
        let currentSlideIndex = 0;
        let scrollTimeout; // To debounce scroll updates

        function updateNavigation() {
            // Calculate current slide based on scroll position more reliably
            const containerHeight = presentationContainer.clientHeight;
            const scrollCenter = presentationContainer.scrollTop + containerHeight / 2;

            let closestSlideIndex = 0;
            let minDistance = Infinity;

            slides.forEach((slide, index) => {
                const slideCenter = slide.offsetTop + slide.offsetHeight / 2;
                const distance = Math.abs(scrollCenter - slideCenter);
                if (distance < minDistance) {
                    minDistance = distance;
                    closestSlideIndex = index;
                }
            });

            currentSlideIndex = closestSlideIndex;

            slideCounter.textContent = `${currentSlideIndex + 1} / ${totalSlides}`;
            prevBtn.disabled = currentSlideIndex === 0;
            nextBtn.disabled = currentSlideIndex === totalSlides - 1;
        }

        function navigateSlides(direction) {
            const nextSlideIndex = currentSlideIndex + direction;
            if (nextSlideIndex >= 0 && nextSlideIndex < totalSlides) {
                slides[nextSlideIndex].scrollIntoView({ behavior: 'smooth', block: 'start' });
                // Update index immediately for responsiveness, scroll listener will confirm
                currentSlideIndex = nextSlideIndex;
                updateNavigation();
            }
        }

        // Update navigation on scroll (debounced)
        presentationContainer.addEventListener('scroll', () => {
            clearTimeout(scrollTimeout);
            scrollTimeout = setTimeout(updateNavigation, 50); // Adjust delay as needed
        });

        // Keyboard navigation
        document.addEventListener('keydown', (event) => {
            // Check if focus is inside a text input/textarea to avoid interference
            const activeElement = document.activeElement;
            const isInputFocused = activeElement.tagName === 'INPUT' || activeElement.tagName === 'TEXTAREA';

            if (isInputFocused) return; // Don't navigate if typing

            if (event.key === 'ArrowDown' || event.key === 'PageDown' || event.key === ' ') { // Space or Right Arrow for next
                event.preventDefault();
                navigateSlides(1);
            } else if (event.key === 'ArrowUp' || event.key === 'PageUp') { // Left Arrow for previous
                event.preventDefault();
                navigateSlides(-1);
            } else if (event.key === 'Home') {
                 event.preventDefault();
                 slides[0].scrollIntoView({ behavior: 'smooth', block: 'start' });
                 currentSlideIndex = 0; // Reset index
                 updateNavigation();
            } else if (event.key === 'End') {
                 event.preventDefault();
                 slides[totalSlides - 1].scrollIntoView({ behavior: 'smooth', block: 'start' });
                 currentSlideIndex = totalSlides - 1; // Reset index
                 updateNavigation();
            }
        });

        // Button listeners
        prevBtn.addEventListener('click', () => navigateSlides(-1));
        nextBtn.addEventListener('click', () => navigateSlides(1));

        // Initial setup
        document.addEventListener('DOMContentLoaded', () => {
            // Set initial state after layout calculation
             setTimeout(updateNavigation, 150); // Delay slightly more for accurate scroll pos on load
        });

        // Update on resize
        window.addEventListener('resize', () => {
             // Recalculate positions and update nav on resize
             clearTimeout(scrollTimeout);
             scrollTimeout = setTimeout(updateNavigation, 100);
        });
    </script>
</body>
</html>
