<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Keynote: The Future of Multimodal AI Applications</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-typescript.min.js" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js" defer></script>


    <style>
        :root {
            --black: #000000;
            --white: #FFFFFF;
            --soft-pink: #FFD6E0;
            --lavender: #E0D6FF;
            --mint-green: #C1F0DB;
            --dark-gray: #333333;
            --medium-gray: #666666;
            --light-gray-bg: #f7f7f7; /* For code blocks and backgrounds */
            --blue-100: #DBEAFE;
            --blue-500: #3B82F6;
            --pink-800: #9D174D; /* Example, adjust if needed */
            --purple-800: #5B21B6; /* Example, adjust if needed */
            --green-800: #065F46; /* Example, adjust if needed */
            --blue-800: #1E40AF; /* Example, adjust if needed */
            --purple-600: #7C3AED; /* Example, adjust if needed */
            --blue-600: #2563EB; /* Example, adjust if needed */
            --blue-700: #1D4ED8; /* Example, adjust if needed */
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            /* Applying DM Sans font */
            font-family: 'DM Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }

        html {
             scroll-behavior: smooth; /* Smooth scrolling between slides */
        }

        body {
            background-color: var(--white);
            color: var(--black);
            line-height: 1.6;
            overflow: hidden; /* Prevent body scroll, container handles it */
        }

        /* Presentation container */
        #presentation-container {
            height: 100vh;
            overflow-y: scroll; /* Enable vertical scrolling */
            scroll-snap-type: y mandatory; /* Snap scrolling to slides */
            position: relative;
        }

        .slide {
            width: 100%;
            min-height: 100vh; /* Ensure slide takes full viewport height */
            padding: 4rem 5%; /* Responsive padding */
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center; /* Center content horizontally */
            scroll-snap-align: start; /* Snap to the start of the slide */
            position: relative;
            overflow: hidden; /* Prevent content overflow issues */
            background-color: var(--white); /* Default background */
        }

        .slide-content {
            max-width: 1200px;
            width: 100%;
            z-index: 1; /* Content above background */
            text-align: left; /* Default text alignment */
        }

        /* Optional subtle gradient background for slides */
        .gradient-bg {
            background: linear-gradient(135deg, rgba(255, 214, 224, 0.1), rgba(224, 214, 255, 0.1), rgba(193, 240, 219, 0.1));
        }

        /* Headings */
        h1, h2, h3, h4, h5, h6 {
            margin-bottom: 1.5rem;
            font-weight: 700;
            line-height: 1.3;
        }

        h1 { /* Title slide main heading - Fixed Size & Color */
            font-size: 3.5rem !important; /* Fixed size override */
            color: var(--dark-gray) !important; /* Set color to dark gray, override */
            display: inline-block;
            margin-bottom: 1rem; /* Adjusted margin */
        }

        h2 { /* Slide Titles - Fixed Size */
            font-size: 3rem !important; /* Fixed size override */
            color: var(--black);
            padding-bottom: 0.5rem;
            position: relative;
            margin-bottom: 2rem; /* More space after title */
        }

        /* Underline effect for h2 */
        h2::after {
             content: '';
             position: absolute;
             bottom: 0;
             left: 0;
             width: 60px;
             height: 4px;
             background: linear-gradient(90deg, var(--soft-pink), var(--lavender), var(--mint-green));
             border-radius: 2px;
         }
         .center-content h2::after {
            left: 50%;
            transform: translateX(-50%);
        }


        h3 { /* Subtitles / Section Heads */
            font-size: clamp(1.5rem, 4vw, 2rem);
            color: var(--dark-gray);
        }

        p, li {
            margin-bottom: 1rem;
            color: var(--dark-gray);
            font-size: clamp(1rem, 2.5vw, 1.2rem); /* Responsive font size */
        }
        ul, ol { /* Added ol styling */
            list-style-position: outside;
            padding-left: 1.5rem;
            margin-bottom: 1rem; /* Added margin bottom */
        }
        li {
            margin-bottom: 0.5rem; /* Reduced li margin */
        }
        strong {
            font-weight: 600; /* Ensure strong is bold */
            color: var(--black);
        }
        blockquote {
            margin-top: 1rem;
            margin-bottom: 1rem;
            padding-left: 1.5rem;
            border-left: 4px solid var(--lavender);
            font-style: italic;
            color: var(--medium-gray);
        }

        /* Grid layout */
        .grid-layout {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); /* Responsive columns */
            gap: 2rem;
            margin: 2rem 0;
        }

        /* Card styling */
        .card {
            background-color: var(--white);
            border-radius: 1rem;
            padding: 2rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            border-left: 4px solid;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.12);
        }
        .card.pink { border-color: var(--soft-pink); }
        .card.purple { border-color: var(--lavender); }
        .card.green { border-color: var(--mint-green); }
        .card.blue {
            /* Using Tailwind gradient for consistency */
            background: linear-gradient(to right, var(--blue-100), #DBEAFE); /* blue-50 to blue-100 */
            border-left: 4px solid var(--blue-500); /* blue-500 */
        }
        .card h3 { margin-top: 0; font-size: 1.5rem; }

        /* Code block styling */
        .code-block {
            margin: 1.5rem 0;
            border-radius: 0.5rem;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        .code-header {
            background-color: var(--dark-gray);
            color: var(--white);
            padding: 0.5rem 1rem;
            font-family: monospace;
            font-size: 0.9rem;
            border-radius: 0.5rem 0.5rem 0 0; /* Match top radius */
        }
        /* Prism overrides */
        pre[class*="language-"] {
            margin: 0 !important;
            padding: 1.5rem !important; /* More padding */
            border-radius: 0 0 0.5rem 0.5rem !important; /* Match bottom radius */
            background-color: var(--light-gray-bg) !important;
            font-size: 0.9rem !important;
            line-height: 1.5 !important;
            max-height: 40vh; /* Limit height and allow scroll */
            overflow: auto !important;
        }
        code[class*="language-"] {
            font-family: 'Fira Code', monospace !important;
            color: var(--dark-gray) !important; /* Adjust text color for light theme */
        }
        /* Adjust Prism theme colors slightly if needed */
        .token.comment, .token.prolog, .token.doctype, .token.cdata { color: slategray; }
        .token.punctuation { color: #999; }
        .token.keyword { color: var(--blue-600); } /* Example adjustment */
        .token.string { color: var(--green-800); } /* Example adjustment */
        .token.function { color: var(--purple-600); } /* Example adjustment */

        /* Image container */
        .image-container {
            width: 100%;
            margin: 2rem 0;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
            display: flex; /* Center image */
            justify-content: center;
            align-items: center;
            background-color: var(--light-gray-bg); /* BG for placeholders */
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            max-height: 50vh; /* Limit image height */
            display: block;
            object-fit: contain; /* Ensure image fits nicely */
        }
        .image-caption { /* Style for captions below images */
            text-align: center;
            font-size: 0.85rem;
            color: var(--medium-gray);
            margin-top: -1rem; /* Pull caption closer */
            margin-bottom: 2rem;
        }

        /* Placeholder Box Styling */
        .placeholder-box {
            border: 2px dashed var(--lavender);
            padding: 2rem;
            text-align: center;
            color: var(--medium-gray);
            border-radius: 0.5rem;
            background-color: rgba(224, 214, 255, 0.1);
            min-height: 200px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            margin: 1rem 0; /* Adjusted margin */
        }

        /* Blueprint diagram styling */
        .blueprint-diagram {
             display: flex;
             flex-direction: column;
             align-items: center;
             gap: 1rem;
             margin: 2rem 0;
        }
        .blueprint-row {
            display: flex;
            justify-content: center;
            gap: 1rem;
            flex-wrap: wrap; /* Allow wrapping on smaller screens */
        }
        .blueprint-item {
            background-color: var(--white);
            border-radius: 0.5rem;
            padding: 1rem 1.5rem;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
            text-align: center;
            min-width: 120px;
            border: 1px solid #eee;
        }
        .blueprint-item strong {
            display: block;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }
        .blueprint-arrow {
            font-size: 1.5rem;
            color: var(--medium-gray);
            align-self: center;
            margin: 0 0.5rem;
        }
        .bp-input { border-left: 4px solid var(--soft-pink); }
        .bp-context { border-left: 4px solid var(--soft-pink); }
        .bp-grounding { border-left: 4px solid var(--lavender); }
        .bp-core { border-left: 4px solid var(--lavender); background-color: rgba(224, 214, 255, 0.1); }
        .bp-output { border-left: 4px solid var(--mint-green); }
        .bp-interface { border-left: 4px solid var(--mint-green); }


        /* Specific slide layouts */
        .title-slide .slide-content {
            text-align: center;
        }
        .title-slide h3 { /* Subtitle on title slide */
            font-size: clamp(1.2rem, 3vw, 1.8rem);
            font-weight: 500;
            color: var(--dark-gray);
            margin-bottom: 1.5rem;
        }
        .title-slide .affiliation {
            font-size: clamp(1.1rem, 3vw, 1.5rem);
            color: var(--dark-gray);
            margin-top: 1rem;
            margin-bottom: 1.5rem;
        }
        .title-slide .conference-info {
             font-size: clamp(0.9rem, 2vw, 1.1rem);
             color: var(--medium-gray);
             margin-top: 2rem;
        }
        .title-slide .schedule-link {
             display: inline-block;
             padding: 0.75rem 1.5rem;
             background: linear-gradient(90deg, var(--soft-pink), var(--lavender));
             color: var(--black);
             text-decoration: none;
             border-radius: 99px; /* Pill shape */
             font-weight: 600;
             margin-top: 1rem;
             transition: transform 0.2s ease, box-shadow 0.2s ease;
             box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
         }
        .title-slide .schedule-link:hover {
             transform: translateY(-3px);
             box-shadow: 0 6px 15px rgba(0, 0, 0, 0.15);
         }

        .two-column-layout {
            display: grid;
            grid-template-columns: 1fr 1fr; /* Two equal columns */
            gap: 3rem;
            align-items: start; /* Align items to top */
            margin-top: 2rem;
        }

        /* References slide styling */
        .references-slide ul {
            list-style-type: none;
            padding-left: 0;
            font-size: 0.8rem;
            line-height: 1.5;
            max-height: 70vh; /* More height for refs */
            overflow-y: auto;
            columns: 2; /* Display references in two columns */
            column-gap: 2rem;
        }
        .references-slide li {
            margin-bottom: 0.5rem;
            word-break: break-word;
        }

        /* Citation styling */
        .citation {
            font-size: 0.8rem;
            color: var(--medium-gray);
            margin-top: 1rem;
            text-align: right;
        }

        /* Navigation Buttons */
        #navigation {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            z-index: 100;
            display: flex;
            gap: 1rem;
        }
        .nav-button {
            background: linear-gradient(to right, var(--soft-pink), var(--lavender), var(--mint-green));
            color: var(--black);
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 9999px; /* Pill shape */
            cursor: pointer;
            font-weight: 600;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            font-size: 1rem;
        }
        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 15px rgba(0, 0, 0, 0.15);
        }
        .nav-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
        #slideCounter { /* Uncommented and styled */
            position: fixed;
            bottom: 2.5rem; /* Align with buttons */
            left: 50%;
            transform: translateX(-50%);
            z-index: 100;
            font-size: 0.9rem;
            color: var(--medium-gray);
            background-color: rgba(255, 255, 255, 0.8);
            padding: 0.25rem 0.75rem;
            border-radius: 99px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            backdrop-filter: blur(2px); /* Optional: blur background */
        }


        /* Responsive adjustments */
        @media (max-width: 768px) {
            .slide {
                padding: 3rem 5%; /* Adjust padding */
                min-height: unset; /* Allow slides to be shorter than viewport */
                 scroll-snap-align: center; /* Center snap on mobile */
            }
            /* Keep fixed h1/h2 sizes as per user request */
             h1 { font-size: 3.5rem !important; }
             h2 { font-size: 3rem !important; }
            h3 { font-size: clamp(1.3rem, 5vw, 1.8rem); }
            p, li { font-size: clamp(0.9rem, 4vw, 1.1rem); }

            .two-column-layout {
                grid-template-columns: 1fr; /* Stack columns */
                gap: 2rem;
            }
            .blueprint-diagram {
                flex-direction: column;
            }
             .blueprint-row {
                 flex-direction: column;
                 width: 100%;
                 align-items: stretch; /* Make items full width */
             }
             .blueprint-arrow {
                transform: rotate(90deg); /* Point arrows down */
                margin: 0.5rem 0;
             }
             .references-slide ul {
                columns: 1; /* Single column on mobile */
                max-height: 60vh;
            }
            #navigation {
                bottom: 1rem;
                right: 1rem;
                gap: 0.5rem;
            }
            .nav-button {
                padding: 0.5rem 1rem;
                font-size: 0.9rem;
            }
            #slideCounter {
                bottom: 1.5rem;
                font-size: 0.8rem;
            }
            .content-with-image { /* Stack image and text on mobile */
                flex-direction: column;
            }
            .content-with-image .image-wrapper {
                 flex-basis: auto; /* Reset basis */
                 width: 80%; /* Control width */
                 margin-top: 1.5rem;
            }
        }

        /* Fullscreen Image Slide Style */
        .fullscreen-image {
            background-size: contain; /* Use contain to ensure the whole image fits */
            background-position: center center;
            background-repeat: no-repeat;
            display: flex;
            flex-direction: column;
            justify-content: flex-end; /* Align title towards bottom */
            align-items: center; /* Center title horizontally */
            background-color: var(--light-gray-bg); /* Fallback BG */
        }
        .fullscreen-image .slide-content {
             padding-bottom: 3rem; /* Space for the title at the bottom */
             width: 100%;
             text-align: center; /* Center title block */
        }
        .fullscreen-image h2 {
            background-color: rgba(0, 0, 0, 0.6); /* Semi-transparent black background */
            color: white !important; /* Force white color */
            padding: 0.5rem 1.5rem;
            border-radius: 8px;
            font-size: 2rem !important; /* Override fixed size if needed */
            text-align: center;
            display: inline-block; /* Ensure background fits text */
            margin-bottom: 0; /* Remove margin below title */
        }
         .fullscreen-image h2::after {
             display: none; /* Hide underline on fullscreen image titles */
         }

         /* Specific style for the first slide */
         #slide-0 {
             /* No background image needed here */
             background-color: var(--white);
         }
         #slide-0 .slide-content {
             display: flex;
             flex-direction: column;
             justify-content: center;
             align-items: center;
             text-align: center;
             height: 100%;
         }
         #slide-0 h1 {
             color: var(--dark-gray) !important; /* Override gradient */
             margin-bottom: 2rem;
         }
         #slide-0 .image-container {
             max-width: 600px; /* Control size of lego image */
             box-shadow: none;
             background-color: transparent;
             margin-top: 1rem;
             margin-bottom: 3rem;
         }
         #slide-0 .image-container img {
             max-width: 100%;
             height: auto;
             display: block;
             margin: 0 auto;
             border-radius: 0.5rem;
         }
         #slide-0 .conference-info {
             position: absolute;
             bottom: 30px;
             left: 50%;
             transform: translateX(-50%);
             width: 100%;
             text-align: center;
         }

    </style>
</head>
<body>
    <div id="presentation-container">

        <section class="slide title-slide" id="slide-0">
            <div class="slide-content">
                <h1>The Future of Multimodal AI Applications</h1>
                <div class="image-container">
                    <img src="/api/placeholder/600/400/E0D6FF/333333?text=Multimodal+Concept" alt="Multimodal Lego Blocks Concept">
                </div>
                <p class="affiliation">
                    <strong>Stefania Druga</strong><br>
                    Research Scientist, Google DeepMind
                </p>
                <a href="https://shift.infobip.com/us/schedule/" target="_blank" class="schedule-link">View Event Schedule</a>
                <p class="conference-info">
                    Infobip Shift Miami | May 6, 2025 <br>
                    Honored to be here alongside such an incredible lineup of speakers.
                </p>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Beyond Text: AI That Perceives the World</h2>
                <p class="text-xl mb-6">Imagine AI that doesn't just process text, but <strong>perceives</strong> the world alongside us – seeing our experiments, hearing our questions, sensing the environment.</p>
                <div class="grid-layout">
                    <div class="card pink">
                        <h3>The Limitation</h3>
                        <p>Current text-centric AI often misses the richness of real-world context and lacks direct perception.</p>
                    </div>
                    <div class="card purple">
                         <h3>The Need</h3>
                         <p>Why AI that sees, hears, senses? To build more intuitive, grounded, and truly helpful systems.</p>
                    </div>
                </div>
                 <div class="mt-8 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
                    <p class="font-semibold text-black text-lg">Example:</p>
                    <p>An AI tutor not just reading a math problem, but *seeing* where a student makes a mistake on the paper, in real-time.</p>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Vision: Real-Time Multimodal AI</h2>
                <div class="content-with-image">
                    <div class="text-content">
                         <p class="text-xl mb-6"> AI systems that seamlessly integrate and synthesize information from diverse, real-time data streams:</p>
                         <ul>
                             <li>Live Webcams & Video Feeds</li>
                             <li>Microphone Audio</li>
                             <li>Connected Sensors (Temperature, Location, etc.)</li>
                             <li>Bespoke Hardware (Jacdac)</li>
                         </ul>
                          <p class="mt-4">Understanding context, anticipating needs, and responding dynamically through multi-sensory feedback loops.</p>
                    </div>
                    <div class="image-wrapper">
                         <img src="/api/placeholder/400/300/C1F0DB/333333?text=Sensor+Integration" alt="Sensors Context Analysis Diagram">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Why Multimodal AI Matters</h2>
                <div class="grid-layout">
                    <div class="card pink">
                        <h3>Beyond Text</h3>
                        <p>Traditional AI systems rely primarily on text, limiting their ability to understand and interact with the rich, multimodal world humans naturally navigate.</p>
                    </div>
                    <div class="card purple">
                        <h3>Real-Time Interaction</h3>
                        <p>The most impactful AI systems don't just analyze - they respond dynamically to changing inputs from multiple streams with minimal latency.</p>
                    </div>
                    <div class="card green">
                        <h3>Grounding</h3>
                        <p>By connecting language to sensory inputs (vision, audio, sensors), multimodal AI anchors abstract concepts in real-world perception, leading to deeper understanding and reduced ambiguity.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Key Technical Challenges</h2>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 lg:grid-cols-4">
                    <div class="card pink">
                        <h3>1. Data Fusion</h3>
                        <p>How to meaningfully combine signals from different sources?</p>
                        <p class="text-sm text-gray-500 mt-2">Analogy: Simultaneous translation + comprehension.</p>
                    </div>
                    <div class="card purple">
                        <h3>2. Latency Reduction</h3>
                        <p>How to process multiple streams fast enough for real-time?</p>
                        <p class="text-sm text-gray-500 mt-2">Analogy: Lag-free video call with effects.</p>
                    </div>
                    <div class="card green">
                         <h3>3. Context Modeling</h3>
                         <p>How to maintain coherent understanding across modalities over time?</p>
                         <p class="text-sm text-gray-500 mt-2">Analogy: Remembering conversation + noticing expressions.</p>
                    </div>
                     <div class="card pink">
                         <h3>4. Interaction Design</h3>
                         <p>How to design intuitive multi-input interactions?</p>
                         <p class="text-sm text-gray-500 mt-2">Analogy: Seamless car dashboard controls.</p>
                    </div>
                </div>
                <p class="mt-8 text-center text-lg text-gray-700">These require innovations in architecture, algorithms, and system design.</p>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>A Blueprint for Multimodal Apps</h2>
                <p class="text-center mb-6">Visualizing the core components of an interactive multimodal system:</p>
                <div class="image-container max-w-2xl mx-auto">
                     <img src="ChatGPT%20Image%20May%205%2C%202025%2C%2002_04_45%20PM.jpg" alt="Multimodal AI Blueprint Diagram showing User, Processor, and Sensor Inputs (Camera, Audio, Data)">
                     <p class="image-caption">Diagram illustrating multimodal inputs (camera, audio, data sensors) connected to a central processor and user interaction.</p>
                </div>
                 <div class="blueprint-diagram">
                     <div class="blueprint-row">
                         <div class="blueprint-item bp-input"><strong>Input</strong><br>(Voice, Text, Gesture, Code)</div>
                         <div class="blueprint-arrow">➡️</div>
                         <div class="blueprint-item bp-context"><strong>Context</strong><br>(Sensors, History, State)</div>
                     </div>
                     <div class="blueprint-arrow">⬇️</div>
                     <div class="blueprint-row">
                          <div class="blueprint-item bp-output"><strong>Output</strong><br>(Speech, Visuals, Actions)</div>
                         <div class="blueprint-arrow">⬅️</div>
                         <div class="blueprint-item bp-core"><strong>AI Core</strong><br>(Fusion, Reasoning, Generation)</div>
                         <div class="blueprint-arrow">⬅️</div>
                         <div class="blueprint-item bp-grounding"><strong>Grounding</strong><br>(Linking Input to Context)</div>
                     </div>
                      <div class="blueprint-arrow">⬆️</div>
                       <div class="blueprint-row">
                            <div class="blueprint-item bp-interface"><strong>Interface / Environment</strong><br>(App, Robot, Editor)</div>
                       </div>
                 </div>
                 <p class="text-center mt-4">These blocks work in a continuous loop, addressing fusion, latency, and context challenges.</p>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2 class="text-center">Case Studies: Bringing the Blueprint to Life</h2>
                <p class="text-xl text-center mb-12">Let's see how this blueprint applies in practice with examples from our research...</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6">
                    <div class="card pink">
                        <h3 class="text-2xl mb-2">ChemBuddy</h3>
                        <p>Making abstract chemistry tangible through real-time sensing and interaction.</p>
                    </div>
                     <div class="card purple">
                        <h3 class="text-2xl mb-2">MathMind</h3>
                        <p>Visually identifying and addressing mathematical misconceptions on the fly.</p>
                    </div>
                     <div class="card green">
                        <h3 class="text-2xl mb-2">Cognimates Copilot</h3>
                        <p>Supporting creative coding beyond text with multimodal assistance.</p>
                    </div>
                    <div class="card blue">
                        <h3 class="text-2xl mb-2">Gemini Smart Home</h3>
                        <p>Conversational control of smart devices directly within the Gemini AI.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Case Study: Gemini & Google Home Integration</h2>
                <div class="content-with-image grid grid-cols-5 gap-8 items-center">
                    <div class="text-content col-span-3">
                        <p class="text-xl mb-4">Bringing natural language smart home control directly into the Gemini AI chat experience.</p>
                        <h3 class="mb-2 text-lg font-semibold">Core Idea:</h3>
                        <ul class="list-disc list-outside space-y-1">
                            <li>Control lights, climate, media, etc. via Gemini prompts.</li>
                            <li>Example: "Set the dining room for a romantic date night."</li>
                            <li>Reduces friction by keeping control within the AI chat context.</li>
                        </ul>
                        <p class="mt-4 text-sm text-gray-600">Project Goal: Make smart home interaction more intuitive and conversational.</p>
                    </div>
                    <div class="image-wrapper col-span-2 flex justify-center items-center max-h-full">
                        <img src="/api/placeholder/400/500/DBEAFE/1E40AF?text=Gemini+Home+UI" alt="Gemini Google Home Extension UI" class="max-h-[50vh] object-contain">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Gemini Home Extension: Details & Early Access</h2>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <h3 class="mb-2">Capabilities & Limitations:</h3>
                        <ul class="list-disc list-outside space-y-1 mb-4">
                            <li><strong>Controls:</strong> Lighting, climate, window coverings, TVs, speakers, etc.</li>
                            <li><strong>Requires Home App for:</strong> Security devices (cameras, locks), Routines.</li>
                            <li><strong>Activation:</strong> May need '@Google Home' in prompts initially.</li>
                            <li><strong>Context:</strong> Part of broader industry trend (cf. Alexa, Siri AI upgrades).</li>
                        </ul>
                    </div>
                    <div class="p-6 rounded-lg bg-white shadow-md border border-gray-200">
                        <h3 class="mb-3 text-lg font-semibold">Sign Up for Public Preview:</h3>
                        <p class="mb-4">Get early access to this feature (Android, English only initially) via the Google Home Public Preview program.</p>
                        <a href="https://support.google.com/googlenest/answer/12494697" target="_blank" class="inline-block bg-blue-600 text-white px-4 py-2 rounded hover:bg-blue-700 transition-colors">
                            Join Google Home Public Preview
                        </a>
                         <p class="text-xs mt-3 text-gray-500">Requires signing into Gemini with the same account as Google Home.</p>
                    </div>
                 </div>
            </div>
        </section>

        <section class="slide gradient-bg">
             <div class="slide-content">
                <h2>Case Study: ChemBuddy</h2>
                <h3 class="text-lg text-gray-600 mb-6">AI-Powered Chemistry Lab Assistant</h3>
                <div class="content-with-image grid grid-cols-5 gap-8 items-center">
                    <div class="text-content col-span-2">
                        <p class="text-xl mb-4">Making abstract chemistry tangible through real-time sensing and interaction.</p>
                        <h3 class="mb-2 text-lg font-semibold">Core Features:</h3>
                        <ul class="list-disc list-outside space-y-1 ml-5 mb-4 text-lg">
                            <li>Real-world pH sensing via Jacdac</li>
                            <li>AI analyzes sensor data & user actions</li>
                            <li>Adaptive guidance based on experiment state</li>
                        </ul>
                        <p class="mt-4 text-md text-gray-600">Goal: Bridge concrete actions with abstract concepts.</p>
                    </div>
                    <div class="image-wrapper col-span-3 flex justify-center items-center max-h-full">
                         <img src="/api/placeholder/600/400/FFD6E0/9D174D?text=ChemBuddy+Overview" alt="ChemBuddy Overview" class="max-h-[60vh] object-contain rounded-lg shadow-md">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide fullscreen-image" style="background-image: url('/api/placeholder/1200/800/FFFFFF/333333?text=ChemBuddy+Experiments+Setup');">
            <div class="slide-content text-center">
                 <h2 class="bg-black bg-opacity-60 text-white px-4 py-2 rounded inline-block">ChemBuddy Experiments Setup</h2>
            </div>
        </section>

        <section class="slide fullscreen-image" style="background-image: url('/api/placeholder/1200/800/FFFFFF/333333?text=ChemBuddy+Grounding+Example');">
            <div class="slide-content text-center">
                 <h2 class="bg-black bg-opacity-60 text-white px-4 py-2 rounded inline-block">ChemBuddy Grounding Example</h2>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>ChemBuddy: Architecture</h2>
                <div class="image-container" style="text-align: center;">
                    <img src="/api/placeholder/800/450/FFFFFF/333333?text=ChemBuddy+Architecture" alt="ChemBuddy Architecture Diagram" style="max-height: 450px;">
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>ChemBuddy: Implementation Highlight</h2>
                <strong class="text-xl block mb-2 text-black">Real-time Loop (Core Logic)</strong>
                <p class="mb-2">Using WebSockets, the client sends sensor/image data; the server fuses it and calls the AI for immediate feedback.</p>
                <div class="code-block">
                    <div class="code-header">ChemBuddy: WebSocket Multimodal Fusion</div>
                    <pre><code class="language-javascript">// Client: Send multimodal update via WebSocket
function sendUpdate(text, imageBlob, sensorData) {
  const payload = { text, sensorContext: formatSensorData(sensorData), hasImage: !!imageBlob };
  socket.send(JSON.stringify({ type: 'chat_message_request', payload }));
  if (imageBlob) socket.send(imageBlob); // Send binary image after JSON
}

// Server: Handle WebSocket message
socket.on('message', async (message) => {
  if (message instanceof Buffer) { // Image data received
    const imageBase64 = message.toString('base64');
    // Combine with pending text/sensor data
    const { text, sensorContext } = getPendingDataForClient(socket.clientId);
    // --> Call Multimodal AI (Gemini) with text, imageBase64, sensorContext
    const aiResponse = await callGeminiApi({ text, imageBase64, sensorContext });
    socket.send(JSON.stringify({ type: 'ai_response', payload: { text: aiResponse } }));
  } else { // Text/JSON data received
    const data = JSON.parse(message.toString());
    if (data.payload.hasImage) {
      storePendingDataForClient(socket.clientId, data.payload); // Wait for image
    } else {
      // --> Call Multimodal AI (Gemini) with text, null image, sensorContext
      const aiResponse = await callGeminiApi(data.payload);
      socket.send(JSON.stringify({ type: 'ai_response', payload: { text: aiResponse } }));
    }
  }
});</code></pre>
                </div>
                 <div class="mt-6 p-6 rounded-lg bg-white border border-gray-200 shadow-sm">
                    <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
                    <p class="text-lg">Integrating real-time sensor data significantly improved the AI's ability to provide relevant, safety-conscious, and conceptually accurate guidance during experiments.</p>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Case Study: MathMind</h2>
                <h3>Visually Identifying Math Misconceptions</h3>
                 <div class="content-with-image">
                     <div class="text-content">
                         <p class="text-xl mb-4">Visually identifying and addressing mathematical misconceptions on the fly.</p>
                         <h3 class="mb-2">Core Features:</h3>
                         <ul>
                             <li>Real-time vision analysis of handwritten work</li>
                             <li>Classification of errors against misconception taxonomy</li>
                             <li>Targeted, multimodal feedback (visual hints, explanations)</li>
                         </ul>
                         <p class="mt-4 text-sm text-gray-600">Goal: Provide timely, personalized scaffolding for math learning.</p>
                     </div>
                     <div class="image-wrapper">
                          <img src="/api/placeholder/500/600/E0D6FF/5B21B6?text=MathMind+Example" alt="MathMind Misconception Example" class="rounded-lg shadow-lg object-contain max-h-[70vh]">
                     </div>
                 </div>
            </div>
        </section>

        <section class="slide fullscreen-image" style="background-image: url('/api/placeholder/1200/800/FFFFFF/333333?text=Math+Misconceptions+Taxonomy');">
            <div class="slide-content text-center">
                 <h2 class="bg-black bg-opacity-60 text-white px-4 py-2 rounded inline-block">Identifying Math Misconceptions</h2>
            </div>
        </section>

        <section class="slide gradient-bg">
             <div class="slide-content">
                 <h2>MathMind: Architecture</h2>
                 <div class="image-container" style="text-align: center;">
                     <img src="/api/placeholder/800/450/FFFFFF/333333?text=MathMind+Architecture" alt="MathMind Architecture Diagram" style="max-height: 450px;">
                 </div>
             </div>
         </section>

        <section class="slide">
            <div class="slide-content">
                <h2>MathMind: Implementation Highlight</h2>
                 <strong class="text-xl block mb-2 text-black">Vision Analysis Workflow (Core Logic)</strong>
                 <p class="mb-2">Send the image and a task-specific prompt (including misconception taxonomy) to a Vision Language Model.</p>
                 <div class="code-block">
                     <div class="code-header">MathMind: Vision API Call for Misconception Detection</div>
                    <pre><code class="language-javascript">// Client: Call backend API to analyze image
async function analyzeMathWork(imageBase64) {
  const response = await fetch('/api/analyze', { /* POST imageBase64 */ });
  const analysis = await response.json();
  displayFeedback(analysis); // Show results in UI
}

// Server: Process request and call AI
app.post('/api/analyze', async (req, res) => {
  const { imageBase64 } = req.body;
  const model = getGeminiVisionModel(); // e.g., gemini-1.5-pro

  // Prepare parts for Gemini API
  const imagePart = { inlineData: { mimeType: 'image/jpeg', data: imageBase64 } };
  const textPart = { text: MATH_MISCONCEPTION_PROMPT }; // Prompt includes taxonomy

  // --> Call Gemini Vision API
  const result = await model.generateContent([textPart, imagePart]);
  const analysis = parseAnalysisResponse(result.response.text()); // Parse JSON response

  res.json(analysis); // Send structured results back
});</code></pre>
                </div>
                <div class="mt-6 p-6 rounded-lg bg-gradient-to-r from-pink-50 via-purple-50 to-green-50 border border-purple-200 shadow-sm">
                    <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
                    <p class="text-lg">Direct visual analysis of student work allows for highly specific and timely misconception detection, enabling the generation of truly personalized feedback and practice.</p>
                </div>
            </div>
        </section>

        <section class="slide fullscreen-image" style="background-image: url('/api/placeholder/1200/800/FFFFFF/333333?text=Cognimates+Copilot+Overview');">
             <div class="slide-content text-center">
                  <h2 class="bg-black bg-opacity-60 text-white px-4 py-2 rounded inline-block">Case Study: Cognimates Copilot</h2>
             </div>
         </section>

        <section class="slide fullscreen-image" style="background-image: url('/api/placeholder/1200/800/FFFFFF/333333?text=Cognimates+Codelab+UI');">
             <div class="slide-content text-center">
                  <h2 class="bg-black bg-opacity-60 text-white px-4 py-2 rounded inline-block">Cognimates Codelab UI</h2>
             </div>
         </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                 <h2>Cognimates Copilot: Implementation Highlight</h2>
                 <strong class="text-xl block mb-2 text-black">AI Call & Response Parsing (Core Logic)</strong>
                 <p class="mb-2">User input triggers an AI call; the response (text, code, or image data) is parsed and integrated back into the coding environment.</p>
                <div class="code-block">
                    <div class="code-header">Cognimates: Multimodal Response Handling</div>
                    <pre><code class="language-javascript">// Core logic when user asks for help or an asset
async function handleUserInput(prompt, context) {
  // --> 1. Call AI Model (e.g., Gemini) with prompt and context
  const aiResponse = await callAIModelAPI(prompt, context);

  // 2. Parse response to determine type (text, code, image)
  const parsed = parseAIResponse(aiResponse);

  // 3. Integrate back into UI
  if (parsed.type === 'image_asset') {
    // --> Optional: Call background removal API/library
    const finalImage = await removeBackground(parsed.imageBase64);
    displayImageAsset(finalImage); // Add to Scratch assets
  } else if (parsed.type === 'code_suggestion') {
    displayCodeSuggestion(parsed.codeBlocks); // Show blocks in UI
  } else {
    displayExplanation(parsed.text); // Show text in chat
  }
}</code></pre>
                </div>
                 <div class="mt-6 p-6 rounded-lg bg-white border border-gray-200 shadow-sm">
                    <strong class="text-xl block mb-2 text-black">Key Insight/Finding:</strong>
                    <p class="text-lg">Multimodal copilots can significantly lower barriers to creative expression in coding by offering contextual help and asset generation directly within the workflow.</p>
                    <p class="mt-2 text-md text-gray-600"><strong>Ethical Touchpoint:</strong> Important to consider user agency and avoid over-reliance, ensuring the AI assists rather than dictates the creative process.</p>
                 </div>
            </div>
        </section>

        <section class="slide">
             <div class="slide-content">
                 <h2>Cognimates: Model Training & Customization</h2>
                 <div class="two-column-layout">
                     <div>
                         <h3 class="mb-3">Initial Training</h3>
                         <p>Early models focused on text-to-image generation for sprites.</p>
                         <img src="/api/placeholder/400/300/FFFFFF/333333?text=Initial+Training" alt="Cognimates Image Training (Initial)" class="rounded-lg shadow-md object-contain mt-4">
                     </div>
                     <div>
                          <h3 class="mb-3">Refined Training</h3>
                         <p>Later iterations improved visual quality and incorporated background removal.</p>
                         <img src="/api/placeholder/400/300/FFFFFF/333333?text=Refined+Training" alt="Cognimates Image Training (Refined)" class="rounded-lg shadow-md object-contain mt-4">
                     </div>
                 </div>
             </div>
         </section>

        <section class="slide fullscreen-image" style="background-image: url('/api/placeholder/1200/800/FFFFFF/333333?text=Cognimates+User+Studies');">
            <div class="slide-content text-center">
                 <h2 class="bg-black bg-opacity-60 text-white px-4 py-2 rounded inline-block">Cognimates Copilot User Studies Summary</h2>
            </div>
        </section>

        <section class="slide gradient-bg">
             <div class="slide-content">
                 <h2>Cognimates Copilot: Features & Simulations</h2>
                 <div class="two-column-layout">
                     <div>
                         <h3 class="mb-3">New Features</h3>
                          <img src="/api/placeholder/500/350/FFFFFF/333333?text=New+Features" alt="Cognimates Copilot: New Features" class="rounded-lg shadow-md object-contain">
                     </div>
                     <div>
                          <h3 class="mb-3">Simulations</h3>
                          <img src="/api/placeholder/500/350/FFFFFF/333333?text=Simulations" alt="Cognimates Copilot: Simulations" class="rounded-lg shadow-md object-contain">
                     </div>
                 </div>
             </div>
         </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Core Patterns: Input Processing Architectures</h2>
                <div class="image-container" style="text-align: center; margin-bottom: 2rem;">
                    <img src="/api/placeholder/800/300/FFFFFF/333333?text=Processing+Architectures" alt="Multimodal Input Processing Architectures" style="max-height: 300px; display: inline-block;" />
                </div>
                <div class="grid-layout grid-cols-1 md:grid-cols-2">
                    <div class="card pink">
                        <h3>Parallel Processing</h3>
                        <p>Process each modality concurrently through specialized pipelines before fusion.</p>
                        <ul>
                            <li>Optimal for diverse modalities</li>
                            <li>Better utilization of system resources</li>
                        </ul>
                    </div>
                    <div class="card purple">
                        <h3>Hierarchical Processing</h3>
                        <p>Progressive abstraction of features across modalities from low to high-level.</p>
                        <ul>
                            <li>Mirrors human perception systems</li>
                            <li>Enables cross-modal transfer</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Core Patterns: Model Selection & Deployment</h2>
                <div class="grid-layout grid-cols-1 md:grid-cols-3">
                    <div class="card pink">
                        <h3>Cloud Models</h3>
                        <p>Higher capability, larger parameter count models accessed via API</p>
                        <ul>
                            <li>Google's Gemini 2.5 Pro</li>
                            <li>OpenAI's GPT-4V</li>
                            <li>Anthropic's Claude 3.5 Sonnet</li>
                        </ul>
                    </div>
                    <div class="card purple">
                        <h3>Edge-Optimized Models</h3>
                        <p>Compact models designed for on-device deployment</p>
                        <ul>
                            <li>SmolVLM (256M-2.2B)</li>
                            <li>Moondream (1.6B)</li>
                            <li>Qwen2VL (1.5B)</li>
                        </ul>
                    </div>
                    <div class="card green">
                        <h3>Specialized Architectures</h3>
                        <p>Models with unique designs for specific requirements</p>
                        <ul>
                            <li>SSMs for ultra-low latency</li>
                            <li>MiniCPM-V for visual reasoning</li>
                            <li>Whisper/Riva for speech</li>
                        </ul>
                    </div>
                </div>
                <div class="image-container" style="text-align: center; margin-top: 2rem;">
                     <img src="/api/placeholder/600/200/FFFFFF/333333?text=Model+Comparison" alt="Model Evaluation Comparison" style="max-height: 200px; display: inline-block;" />
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Core Patterns: Fusion & Context Modeling</h2>
                <div class="image-container" style="text-align: center; margin-bottom: 2rem;">
                     <img src="/api/placeholder/800/300/FFFFFF/333333?text=Fusion+Techniques" alt="Fusion Techniques Visualization" style="max-height: 300px; display: inline-block;" />
                </div>
                <div class="grid-layout grid-cols-1 md:grid-cols-2">
                    <div class="card pink">
                        <h3>Attention-Based Fusion</h3>
                        <p>Using transformer architectures to create dynamic relationships between modalities</p>
                        <ul>
                            <li>Cross-modal attention</li>
                            <li>Self-attention within modalities</li>
                            <li>Contextual feature weighting</li>
                        </ul>
                    </div>
                    <div class="card purple">
                        <h3>Context Management</h3>
                        <p>Maintaining coherent understanding across time and modalities</p>
                        <ul>
                            <li>Knowledge graph integration</li>
                            <li>Retrieval-augmented generation</li>
                            <li>Hierarchical context windows</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Tackling Real-Time Challenges: Latency & Synchronization</h2>
                <p class="text-xl mb-6">Real-time multimodal systems require overcoming critical engineering hurdles:</p>
                <div class="flex flex-col space-y-6">
                    <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6">
                        <div class="card blue">
                            <h3>Low Latency</h3>
                            <ul class="list-disc pl-5 space-y-1">
                                <li>Efficient Models (SmolVLM, SSMs)</li>
                                <li>Optimized Inference (TensorRT-LLM, MLX)</li>
                                <li>Quantization (4-bit, FP8)</li>
                                <li>Edge Computing & Asynchronous Processing</li>
                                <li>API Optimization (Batching, Caching)</li>
                            </ul>
                            <p class="mt-2"><strong>Goal:</strong> Minimize delay for interactive experiences (e.g., &lt;100ms).</p>
                        </div>
                        <div class="card green">
                            <h3>Data Synchronization</h3>
                            <p>Crucial for coherent understanding (e.g., 32-45ms window)</p>
                            <ul class="list-disc pl-5 space-y-1">
                                <li>Algorithms: Time/Feature/Model-based Alignment</li>
                                <li>Adaptive Temporal Mapping (Handles Jitter)</li>
                                <li>Cross-Modal Correlation Detection</li>
                                <li>Unified Data Platforms (Minimize Integration Sync Issues)</li>
                            </ul>
                             <p class="mt-2"><strong>Goal:</strong> Ensure temporal alignment across streams despite network variability.</p>
                        </div>
                    </div>
                    <div class="image-container flex justify-center mt-4">
                         <img src="/api/placeholder/700/300/FFFFFF/333333?text=Latency+vs+Sync" alt="Graph showing latency spikes and synchronization" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                         <p class="image-caption">Visualizing the complexities of latency and synchronization.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Architectures for Parallel Processing</h2>
                <p>Handling multiple data streams efficiently often requires parallel processing strategies:</p>
                 <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                    <div>
                        <ul class="list-disc pl-5 space-y-2 text-lg">
                            <li><strong>Modality-Specific Pipelines:</strong> Dedicated processing paths before fusion.</li>
                            <li><strong>Asynchronous Task Handling:</strong> Background tasks for non-critical processing.</li>
                            <li><strong>Hardware Acceleration:</strong> Leveraging GPUs, TPUs, etc.</li>
                            <li><strong>Distributed Systems / Edge Computing:</strong> Processing closer to the source.</li>
                            <li><strong>Optimized Scheduling:</strong> Managing compute resources efficiently.</li>
                        </ul>
                        <p class="mt-4"><strong>Goal:</strong> Maximize throughput and responsiveness by handling concurrent data streams effectively.</p>
                    </div>
                    <div class="image-container">
                         <img src="/api/placeholder/500/400/FFFFFF/333333?text=Parallel+Processing" alt="Parallel Processing Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                         <p class="image-caption">Conceptual model of parallel data stream processing.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Evaluating Multimodal Systems: General Benchmarks</h2>
                <p>Assessing performance requires comprehensive benchmarks and rigorous testing:</p>
                 <div class="grid-layout grid-cols-1 md:grid-cols-2 gap-6 items-start">
                     <div class="card pink">
                        <h3>Key Benchmarks</h3>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>MMMU, MathVista, MMStar (General)</li>
                            <li>DocVQA, TextVQA (Documents)</li>
                            <li>Video-MME, CinePile (Video)</li>
                            <li>Domain-Specific (e.g., Healthcare, Robotics)</li>
                        </ul>
                        <p class="mt-2">Evaluating capabilities across diverse tasks and modalities.</p>
                    </div>
                    <div class="card purple">
                        <h3>Testing Strategies</h3>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Component & End-to-End Testing</li>
                            <li>Real-world Scenario Simulation</li>
                            <li>Robustness Testing (Noise, Adversarial)</li>
                            <li>Monitoring & Feedback Loops</li>
                            <li>Measuring Latency, Throughput, Resources</li>
                        </ul>
                         <p class="mt-2">Ensuring reliability beyond standard metrics.</p>
                    </div>
                </div>
                 <div class="image-container grid grid-cols-2 gap-4 mt-6">
                     <img src="/api/placeholder/400/300/FFFFFF/333333?text=Evaluation+Framework" alt="Evaluation Metrics" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                     <img src="/api/placeholder/400/300/FFFFFF/333333?text=Benchmark+Example" alt="Copilot Benchmark Example" class="rounded-lg shadow-lg object-contain max-h-[30vh]">
                 </div>
                 <p class="text-center">Challenges include metrics reflecting real user experience and testing complex interactions.</p>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Beyond General Benchmarks: Domain-Specific Evaluation</h2>
                <p class="text-lg mb-4">Truly understanding co-scientist performance requires evaluation tailored to specific application areas and user goals.</p>
                <div class="two-column-layout items-center gap-8">
                    <div>
                        <strong class="text-xl block mb-2 text-black">Why Domain-Specific?</strong>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Captures nuances missed by broad tests.</li>
                            <li>Aligns evaluation with real-world tasks and workflows.</li>
                            <li>Allows measurement of context-specific understanding (e.g., scientific misconceptions, coding patterns).</li>
                            <li>Provides more actionable insights for improvement.</li>
                        </ul>
                        <strong class="text-xl block mt-4 mb-2 text-black">Example: Math & Science Education</strong>
                        <p>Developing benchmarks specifically focused on identifying and addressing common mathematical or scientific misconceptions demonstrated by students.</p>
                    </div>
                    <div class="flex justify-center items-center">
                         <img src="/api/placeholder/500/400/FFFFFF/333333?text=MathMind+Evaluation" alt="MathMind Evaluation Example showing student work analysis" class="rounded-lg shadow-lg object-contain max-h-[55vh]">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Evaluating Creative Coding Copilots</h2>
                <p class="text-lg mb-4">Assessing AI assistance in open-ended creative tasks requires different approaches than traditional benchmarks.</p>
                <div class="two-column-layout items-center gap-8">
                     <div>
                        <strong class="text-xl block mb-2 text-black">Challenges & Approaches:</strong>
                        <ul class="list-disc pl-5 space-y-1">
                            <li>Defining "success" in creative tasks is subjective.</li>
                            <li>Need to evaluate the *process* as much as the *product*.</li>
                            <li>Measuring impact on user learning, exploration, and overcoming blocks.</li>
                            <li>Developing benchmarks that simulate real coding scenarios (e.g., completing a partial project, debugging, generating specific assets).</li>
                        </ul>
                        <strong class="text-xl block mt-4 mb-2 text-black">Example: Cognimates Copilot Evaluation</strong>
                        <p>Using project-based scenarios to evaluate the copilot's ability to provide relevant code suggestions, explain concepts, and generate useful visual assets within the Scratch environment.</p>
                    </div>
                    <div class="flex justify-center items-center">
                         <img src="/api/placeholder/500/400/FFFFFF/333333?text=Copilot+Benchmark" alt="Cognimates Copilot Evaluation Benchmark Example" class="rounded-lg shadow-lg object-contain max-h-[55vh]">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Standardizing AI Tutor Evaluation: A Pedagogical Framework</h2>
                <p class="text-lg mb-4">The <a href="https://github.com/kaushal0494/UnifyingAITutorEvaluation" target="_blank" class="text-purple-600 hover:underline">Unifying AI Tutor Evaluation</a> framework proposes a taxonomy to assess the pedagogical abilities of LLM-based tutors across key dimensions.</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-start">
                    <div class="space-y-4">
                         <p>This structured approach uses a detailed JSON format to capture tutor responses and annotate them across dimensions like mistake identification, guidance quality, coherence, and tone.</p>
                         <img src="/api/placeholder/500/400/FFFFFF/333333?text=Evaluation+Framework" alt="Diagram illustrating the AI Tutor Evaluation Framework components" class="rounded-lg shadow-lg object-contain max-h-[45vh] mt-4">
                    </div>
                    <div class="code-block max-h-[65vh] overflow-y-auto">
                        <div class="code-header">Evaluation Structure (Simplified JSON)</div>
                        <pre><code class="language-json">{
  "conversation_id": "...",
  "conversation_history": "...",
  "Ground_Truth_Solution": "...",
  "anno_llm_responses": {
    "Model_Name": {
      "response": "Tutor response...",
      "annotation": {
        "Mistake_Identification": "Yes/No/...",
        "Mistake_Location": "Yes/No/...",
        "Revealing_of_the_Answer": "Yes/No/...",
        "Providing_Guidance": "Yes/No/...",
        "Actionability": "Yes/No/...",
        "Coherence": "Yes/No/...",
        "Tutor_Tone": "Encouraging/Neutral/...",
        "Humanlikeness": "Yes/No/..."
      }
    }
  }
}</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>User Research & Testing in Multimodal AI</h2>
                 <p class="text-lg mb-4">Understanding user interaction and experience is critical, especially in Human-Robot Interaction (HRI) and copilots:</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-start">
                    <div class="space-y-3">
                        <ul class="list-disc list-outside ml-5 text-lg">
                            <li><strong>Observational Studies:</strong> Analyzing how users interact naturally.</li>
                            <li><strong>Task-Based Evaluations:</strong> Measuring success rates, efficiency, errors.</li>
                            <li><strong>Qualitative Feedback:</strong> Interviews, surveys for perception, satisfaction.</li>
                            <li><strong>Analyzing Non-Verbal Cues:</strong> Understanding user state (engagement, confusion).</li>
                            <li><strong>Iterative Design:</strong> Incorporating feedback into development cycles.</li>
                        </ul>
                        <p class="mt-4 font-medium text-lg"><strong>Goal:</strong> Build systems that are intuitive, effective, and meet user needs in real-world contexts.</p>
                    </div>
                    <div class="image-container">
                         <img src="/api/placeholder/500/400/FFFFFF/333333?text=User+Testing+Cycle" alt="User Research and Testing Cycle Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                    </div>
                </div>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2>Training & Customization: Synthetic Data & Fine-tuning</h2>
                <p>Adapting models for specific tasks or domains often involves specialized training techniques:</p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                    <div>
                        <ul class="list-disc pl-5 space-y-2 text-lg">
                            <li><strong>Fine-tuning:</strong> Adapting pre-trained models on domain-specific datasets.</li>
                            <li><strong>Synthetic Data Generation:</strong> Creating artificial data to augment limited real-world data.</li>
                            <li><strong>Few-Shot / Zero-Shot Learning:</strong> Enabling tasks with minimal/no examples.</li>
                            <li><strong>Custom Sensor Models:</strong> Training smaller models on specific sensor inputs.</li>
                            <li><strong>PEFT (LoRA, etc.):</strong> Adapting large models efficiently.</li>
                        </ul>
                        <p class="mt-4"><strong>Goal:</strong> Improve performance, handle data scarcity, enable edge deployment.</p>
                    </div>
                    <div class="image-container">
                         <img src="/api/placeholder/500/400/FFFFFF/333333?text=Training+Pipeline" alt="Cognimates Model Training Diagram" class="rounded-lg shadow-lg object-contain max-h-[50vh]">
                         <p class="image-caption">Conceptual example of training pipelines.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Emerging Applications: Beyond the Desktop</h2>
                <p>Real-time multimodal AI is enabling new experiences in physical and immersive environments:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-3 gap-4">
                    <div class="card pink">
                        <h3>Extended Reality (XR)</h3>
                        <p>Enhanced interaction via gesture/gaze understanding, immersive training, context-aware agents (e.g., Meta Ray-Ban AI).</p>
                    </div>
                    <div class="card purple">
                        <h3>Robotics</h3>
                        <p>Improved HRI (non-verbal cues), robust navigation (sensor fusion), semantic scene understanding.</p>
                    </div>
                     <div class="card green">
                        <h3>Smart Cities/IoT</h3>
                        <p>Intelligent transportation, public safety, environmental monitoring using diverse sensor networks.</p>
                    </div>
                </div>
                <div class="image-container mt-6">
                     <img src="/api/placeholder/600/350/DBEAFE/1E40AF?text=Gemini+Home+UI" alt="Google Home Gemini Extension Example" class="rounded-lg shadow-lg object-contain max-h-[40vh]">
                     <p class="image-caption">Example: Integrating powerful multimodal models into home devices.</p>
                </div>
                <p class="text-center">Also impacting Healthcare (Diagnostics), Finance (Fraud Detection), Retail (Personalization), and more.</p>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content text-center">
                <h2>Future Application: Accelerating Material Science Discovery</h2>
                <div class="image-container mt-8">
                    <img src="/api/placeholder/800/600/FFFFFF/333333?text=Material+Science+AI" alt="AI analyzing material structures" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">Multimodal AI can analyze experimental data, simulations, and literature to predict properties of novel materials.</p>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content text-center">
                <h2>Future Application: AI-Assisted CAD & Engineering Design</h2>
                <div class="image-container mt-8">
                     <img src="/api/placeholder/800/600/FFFFFF/333333?text=CAD+AI+Assistant" alt="AI assisting with CAD software" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">AI agents can understand design sketches, suggest optimizations, and automate routine CAD tasks based on multimodal input.</p>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content text-center">
                <h2>Future Application: Enhancing Cultural Heritage Preservation</h2>
                <div class="image-container mt-8">
                     <img src="/api/placeholder/800/600/FFFFFF/333333?text=Cultural+Heritage+AI" alt="AI helping analyze cultural artifacts" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">Analyzing artifacts, translating ancient texts, and creating immersive virtual reconstructions using multimodal data.</p>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content text-center">
                <h2>Future Application: Documenting & Revitalizing Languages</h2>
                <div class="image-container mt-8">
                     <img src="/api/placeholder/800/600/FFFFFF/333333?text=Language+AI" alt="AI assisting with language documentation" class="rounded-lg shadow-xl object-contain max-h-[70vh] mx-auto">
                </div>
                <p class="mt-4 text-lg">Using audio, video, and text to document endangered languages, create learning tools, and facilitate translation.</p>
            </div>
        </section>

        <section class="slide">
            <div class="slide-content">
                <h2 class="text-center">The Future Trajectory: What's Next?</h2>
                <p class="text-center text-xl mb-10">The principles behind these examples point towards a broader future for real-time multimodal AI:</p>
                <div class="grid-layout grid-cols-1 md:grid-cols-2 lg:grid-cols-3">
                    <div class="card pink">
                        <h3 class="text-xl">Personalized Assistance</h3>
                        <p>AI understanding context (location, activity, sensors) for proactive help.</p>
                    </div>
                     <div class="card purple">
                        <h3 class="text-xl">Accessibility Tools</h3>
                        <p>Real-time translation between visual, audio, and haptic information.</p>
                    </div>
                     <div class="card green">
                        <h3 class="text-xl">Robotics</h3>
                        <p>Machines perceiving, understanding, and interacting naturally and safely.</p>
                    </div>
                     <div class="card pink">
                         <h3 class="text-xl">Creative Tools</h3>
                         <p>AI partners collaborating via sketches, gestures, voice, code.</p>
                    </div>
                    <div class="card purple">
                        <h3 class="text-xl">Enhanced Learning</h3>
                        <p>Truly adaptive education responding to diverse styles and real-time needs.</p>
                    </div>
                    <div class="card green">
                        <h3 class="text-xl">Human-AI Collaboration</h3>
                        <p>Richer, intuitive partnerships via shared perception.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="slide gradient-bg">
            <div class="slide-content">
                <h2>Enabling Richer Human-AI Collaboration</h2>
                <p class="mb-8 text-xl">Multimodal AI represents a fundamental shift – moving from AI that processes information <strong>about</strong> the world to AI that can participate <strong>in</strong> the world alongside us.</p>

                <div class="mb-10 p-6 rounded-lg bg-white shadow-md border-l-4 border-purple-400">
                    <strong class="block mb-3 text-xl text-black">Researcher's Perspective (Google DeepMind):</strong>
                    <blockquote class="italic text-gray-700 text-lg">
                        "One of the most exciting frontiers is developing models that not only fuse data but build robust internal representations of the world that persist and update over time... Tackling challenges like long-term context, causal reasoning across modalities, and ensuring safe, reliable real-world interaction remains a core focus."
                    </blockquote>
                </div>

                 <strong class="block mb-3 text-xl text-black">Call to Action & Inspiration:</strong>
                <ul class="list-disc list-outside space-y-2 text-lg ml-5">
                    <li><strong>Think Multimodally:</strong> How can integrating diverse data streams enhance your work?</li>
                    <li><strong>Experiment:</strong> Leverage available tools and platforms to explore capabilities.</li>
                    <li><strong>Consider the Interaction:</strong> How can we design systems that feel natural across senses?</li>
                </ul>
                 <p class="mt-8 text-xl">Let's build AI that leverages a richer understanding of our shared physical world.</p>
            </div>
        </section>

         <section class="slide title-slide">
            <div class="slide-content">
                <h1 class="!text-black !bg-none">Thank You & Q&A</h1> <h3>Let's Discuss the Future of Multimodal AI</h3>

                <div class="mt-10 text-xl text-black">
                    <p><strong>Stefania Druga</strong></p>
                    <p>Research Scientist, Google DeepMind</p>
                    <p class="mt-4">Contact/Links: [Your Contact Info / Linktree / Project Page Here]</p>
                    <p>Conference Schedule: <a href="https://shift.infobip.com/us/schedule/" target="_blank" class="text-purple-600 hover:underline">shift.infobip.com/us/schedule/</a></p>
                </div>

                 <p class="mt-12 text-lg text-gray-700">
                    It's been a pleasure sharing these ideas. Looking forward to the conversations this week with all the brilliant minds gathered here at Infobip Shift – truly inspiring company!
                 </p>
            </div>
        </section>

        <section class="slide references-slide">
             <div class="slide-content">
                <h2>References</h2>
                <p class="text-sm mb-6">(Selected references - list can be expanded)</p>
                <ul>
                    <li>ArXiv. (2024). Multimodality of AI for Education: Towards Artificial General Intelligence.</li>
                    <li>ArXiv. (2024). Multimodal Alignment and Fusion: A Survey.</li>
                     <li>Cartesia Raises $27 Million to Build the Next Generation of Real-Time AI Models - PRWeb (2025)</li>
                    <li>GM Insights. (2025). Multimodal AI Market Size & Share, Statistics Report 2025-2034.</li>
                     <li>LiveKit Blog. (2024). An open source stack for real-time multimodal AI.</li>
                    <li>MIT Technology Review. (2024). Multimodal: AI's new frontier.</li>
                     <li>Mobius Labs - Efficient Multimodal AI for Enterprise Applications (EliteAI Tools)</li>
                    <li>Multimodal Fusion Artificial Intelligence Model to Predict Risk for MACE and Myocarditis... (PMC, 2024)</li>
                    <li>Nature. (2025). On opportunities and challenges of large multimodal foundation models in education.</li>
                     <li>NVIDIA Riva - Speech and Translation AI (NVIDIA)</li>
                     <li>ResearchGate. (2025). SmolVLM: Redefining small and efficient multimodal models.</li>
                    <li>Science Direct. (2025). Taking the next step with generative artificial intelligence: The transformative role of multimodal large language models in science education.</li>
                     <li>U.S. Department of Education. (2024). AI Report.</li>
                     <li>World Economic Forum. (2024). The future of learning: AI is revolutionizing education 4.0.</li>
                     <li>Zilliz. (2024). Top 10 Multimodal AI Models of 2024.</li>
                     <li>Druga, S. et al. (Relevant publications for ChemBuddy, MathMind, Cognimates - *Add Specific Citations*)</li>
                     <li>Advances in Computer AI-assisted Multimodal Data Fusion Techniques (ResearchGate, 2024)</li>
                     <li>Real-Time Multimodal Signal Processing for HRI in RoboCup... (ResearchGate, 2025)</li>
                     <li>Designing the User Interface for Multimodal Speech and Pen-Based Gesture Applications... (ResearchGate)</li>
                     <li>SmolVLM: Redefining small and efficient multimodal models (arXiv, 2025)</li>
                     <li>USER-VLM 360°: Personalized Vision Language Models... (arXiv, 2025)</li>
                     <li>Retrieval Augmented Generation and Understanding in Vision... (arXiv, 2025)</li>
                     <li>GraphRAG with MongoDB Atlas... (MongoDB Blog, 2024)</li>
                     <li>An open source stack for real-time multimodal AI (LiveKit Blog, 2024)</li>
                     <li>Build Real-Time Multimodal XR Apps with NVIDIA AI Blueprint... (NVIDIA Blog, 2024)</li>
                     <li>AIRLab-POLIMI/ROAMFREE (GitHub)</li>
                     <li>MPE™ IMU & Sensor Fusion Software Solutions (221e)</li>
                     <li>Development of an artificial intelligence-based multimodal diagnostic system for early detection of biliary atresia (PMC, 2024)</li>
                     <li>MuDoC: An Interactive Multimodal Document-grounded Conversational AI System (arXiv, 2025)</li>
                     <li>Kaushal, V., et al. (2024). Unifying AI Tutor Evaluation... GitHub.</li>

                </ul>
             </div>
        </section>

    </div> <div id="navigation">
        <button id="prevBtn" class="nav-button" title="Previous Slide (Up Arrow)">▲</button>
        <button id="nextBtn" class="nav-button" title="Next Slide (Down Arrow)">▼</button>
    </div>
    <div id="slideCounter"></div>

    <script>
        const presentationContainer = document.getElementById('presentation-container');
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');
        const slideCounter = document.getElementById('slideCounter');
        let currentSlideIndex = 0;
        let scrollTimeout; // To debounce scroll updates

        function updateNavigation() {
            // Calculate current slide based on scroll position more reliably
            const containerHeight = presentationContainer.clientHeight;
            const scrollCenter = presentationContainer.scrollTop + containerHeight / 2;

            let closestSlideIndex = 0;
            let minDistance = Infinity;

            slides.forEach((slide, index) => {
                const slideCenter = slide.offsetTop + slide.offsetHeight / 2;
                const distance = Math.abs(scrollCenter - slideCenter);
                if (distance < minDistance) {
                    minDistance = distance;
                    closestSlideIndex = index;
                }
            });

            currentSlideIndex = closestSlideIndex;

            slideCounter.textContent = `${currentSlideIndex + 1} / ${totalSlides}`;
            prevBtn.disabled = currentSlideIndex === 0;
            nextBtn.disabled = currentSlideIndex === totalSlides - 1;
        }

        function navigateSlides(direction) {
            const nextSlideIndex = currentSlideIndex + direction;
            if (nextSlideIndex >= 0 && nextSlideIndex < totalSlides) {
                slides[nextSlideIndex].scrollIntoView({ behavior: 'smooth', block: 'start' });
                // Update index immediately for responsiveness, scroll listener will confirm
                currentSlideIndex = nextSlideIndex;
                updateNavigation();
            }
        }

        // Update navigation on scroll (debounced)
        presentationContainer.addEventListener('scroll', () => {
            clearTimeout(scrollTimeout);
            scrollTimeout = setTimeout(updateNavigation, 50); // Adjust delay as needed
        });

        // Keyboard navigation
        document.addEventListener('keydown', (event) => {
            // Check if focus is inside a text input/textarea to avoid interference
            const activeElement = document.activeElement;
            const isInputFocused = activeElement.tagName === 'INPUT' || activeElement.tagName === 'TEXTAREA';

            if (isInputFocused) return; // Don't navigate if typing

            if (event.key === 'ArrowDown' || event.key === 'PageDown' || event.key === ' ') {
                event.preventDefault();
                navigateSlides(1);
            } else if (event.key === 'ArrowUp' || event.key === 'PageUp') {
                event.preventDefault();
                navigateSlides(-1);
            } else if (event.key === 'Home') {
                 event.preventDefault();
                 slides[0].scrollIntoView({ behavior: 'smooth', block: 'start' });
                 currentSlideIndex = 0; // Reset index
                 updateNavigation();
            } else if (event.key === 'End') {
                 event.preventDefault();
                 slides[totalSlides - 1].scrollIntoView({ behavior: 'smooth', block: 'start' });
                 currentSlideIndex = totalSlides - 1; // Reset index
                 updateNavigation();
            }
        });

        // Button listeners
        prevBtn.addEventListener('click', () => navigateSlides(-1));
        nextBtn.addEventListener('click', () => navigateSlides(1));

        // Initial setup
        document.addEventListener('DOMContentLoaded', () => {
            Prism.highlightAll(); // Initial highlight
            // Set initial state after layout calculation
             setTimeout(updateNavigation, 150); // Delay slightly more for accurate scroll pos on load
        });

        // Update on resize
        window.addEventListener('resize', () => {
             // Recalculate positions and update nav on resize
             clearTimeout(scrollTimeout);
             scrollTimeout = setTimeout(updateNavigation, 100);
        });

    </script>
</body>
</html>
